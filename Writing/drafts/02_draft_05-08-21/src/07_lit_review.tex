\documentclass[../01_main.tex]{subfiles}

\begin{document}

\section{Background and Related Work}

This section provides background information on time series and ECGs, as well
as methods to analyze them. First, time series analysis will be covered,
focusing on different time series representation methods and the SAX
representation. Then, ECGs and their analysis will be discussed, focusing on
what an ECG is, computerized ECG analysis, and ECG databases.

\subsection{Time Series and Time Series Analysis}\label{07:time-series-intro}

This subsection will provide background information on time series and time series
analysis methods. A time series is a set of values recorded at specific times. 
A common form of time series are discrete-time time series (often simply called 
discrete time series). Discrete time series are time series whose values are 
recorded at discrete points in time; the most common example of this are time 
series with values recorded at fixed intervals. Continuous-time time series are 
time series that are recorded continuously over a certain interval 
\mycite{brockwell2016}. Time series that contain a single value for each moment
in time are called univariate time series, while time series that record
multiple values at each moment in time are called multivariate time series
\mycite{anacleto2020}. Time series are used in many disciplines to record 
information on time-dependent 
processes, e.g. stock prices in economics, the sun's activity in physics, or 
the heart's activity in medicine. Time series can be recorded digitally,
physically, or, if they were recorded physically, can later be digitized. The 
recorded data can then be used to gain insight into the processes that were
studied. To gain insight using a time series, the relevant information needs to 
be extracted from it--a process that is often called data mining. Data mining 
of time series is a vast discipline that, among others, includes 
\mycite{lin2003,aghabozorgi2015}: 
\begin{itemize}
    \item visualization (graphical representation),
    \item forecasting (predicting future behavior), 
    \item indexing (finding the most similar time series to a given one), 
    \item clustering (dividing time series into groups of similar ones), 
    \item anomaly detection (detecting parts that are not ``normal" or do not
        fit certain parameters), 
    \item classification (assigning a label based on its features, e.g. ``sick"
        and ``not sick"), and
    \item summarization (reducing the complexity--often length--while
        preserving important features).
\end{itemize}
\noindent
Challenges for time series analysis include the often very large datasets that
are difficult for humans to analyze and take up considerable digital storage
space. Analyzing very large datasets requires a large amount of computational 
power because most data mining algorithms become less efficient with larger
datasets \mycite{lin2003}. To mitigate this issue, time series dimension
reduction (also known as dimensionality reduction or time series
representation) is used. Dimension reduction transforms a ``raw" (unmodified) 
time series into a representation that is simpler but resembles the 
raw time series. This can be achieved by either using a method that reduces the
number of values in a time series or by extracting only the relevant features
from the time series. According to \mycite{aghabozorgi2015,shieh2008}, there 
are four types of dimension reduction methods:
\begin{enumerate}
    \item data dictated,
    \item \label{07:dr-methods:01} non-data adaptive,
    \item model-based, and
    \item \label{07:dr-methods:03} data adaptive.
\end{enumerate}
Methods \ref{07:dr-methods:01}--\ref{07:dr-methods:03} have their dimension
reduction factors set by user-defined parameters. This means that the user can
determine how much the dimension of the data should be reduced
\mycite{aghabozorgi2015}.

\subsubsection{Time Series Representation Methods}

\paragraph{Data dictated representation}

Data dictated methods derive their compression ratios from the data 
automatically, the most common form of this method is the clipped
representation \mycite{aghabozorgi2015}. This representation simply transforms
the raw time series into a sequence of 1s and 0s. A data point is assigned
a 1 if its value is larger than the mean value of the time series, and
a 0 otherwise. A sequence of 1s and 0s can be further compressed using various
methods from computer science, finally yielding a very large compression ratio
of 1057:1 \mycite{ratanamahatana2005}.

\paragraph{Non-data adaptive representation}

Non-data adaptive methods operate on time series segments with a fixed size to
reduce the dimension and they are useful for comparing multiple time series with
each other. These methods include the Discrete Wavelet Transform (DWT), the 
Discrete Fourier Transform (DFT), and the Piecewise Aggregate Approximation
(PAA) \mycite{aghabozorgi2015}. The DWT uses wavelets, a limited-duration wave
with an average value of 0, which represents both time 
and frequency information. The DWT is calculated using a series of filters
applied to the signal. In \mycite{kaur2016}, the DWT is used to
detect beats in ECG signals and achieves a 0.221\% detection error rate. The
Fast Fourier Transform, an optimized form of the DFT, decomposes its input
signal into many sinus waves of different frequencies. In \mycite{prasad2018} 
it is used in conjunction with a machine learning model to achieve a beat 
classification accuracy of 98.7\%. The PAA is part of the process of the SAX
representation, thus it will be covered in section \ref{08:paa-proc}.

\paragraph{Model-based representation}

Model-based methods use stochastic methods such as Hidden Markov Models (HMM) 
and the Auto-Regressive Moving Average (ARMA) \mycite{aghabozorgi2015}. An HMM
was used in \mycite{panuccio2002} to cluster electroencephalograph recordings 
(measuring the brain's electrical activity). It was found that their method
was competitive with other established methods in classifying 
electroencephalograph signals. An auto-regressive model can be used to
correctly identify a specific type of arrhythmia in an ECG and to group the
occurrences of this arrhythmia \mycite{corduas2008}.

\paragraph{Data-adaptive representation}

Data adaptive methods use non-fixed size segments and aim to fit the raw data 
most closely. Examples of data-adaptive methods are the Piecewise Polynomial 
Approximation (PPA), Piecewise Linear Approximation (PLA), and SAX 
\mycite{aghabozorgi2015}. PPA can be used to 
compress an ECG by approximating it using polynomials. With second-order 
polynomials, ECGs can be compressed with a minimal level of distortion 
\mycite{nygaard1998}. The authors of \mycite{zhu2018} use a modified PLA 
representation with adaptive ECG segmentation to successfully reconstruct the 
12 standard leads of an ECG from only 3 leads. The SAX representation will be 
covered in detail in 
section \ref{08:sax}. The following subsection \ref{07:section_sax} will 
provide background on the method and its variations.

\subsubsection{SAX Representation Method}\label{07:section_sax}

A particular dimension reduction method is SAX. Introduced by 
\citeauthor{lin2003}, SAX is a symbolic time series representation method for
univariate time series.
The authors felt that the symbolic methods available in \citeyear{lin2003} did 
not provide the desired dimension reduction, did not correspond to the raw data 
accurately enough, and could not be applied to a subset of the total data. 
SAX uses the averaging of a user-defined number of segments and the labeling of
segments with letters to reduce the dimension of the time series data. The
number of letters, called the alphabet size, can also be chosen by the user and
influences the dimension reduction. The
distance between two time series in the SAX representation is guaranteed to 
resemble the distance between the two raw time series, this is called the
distance measure. Since its creation, SAX has found widespread use in data 
mining and many researchers have attempted to modify and improve it.\par

The SAX distance measure has been improved to include the standard deviation
\mycite{zan2016} and a measure of the trend of each averaged segment
\mycite{sun2014,yu2019}. Extended SAX modifies SAX to include the minimum and 
maximum values of each segment for improved representation of the raw data
\mycite{lkhagva2006} while 1d-SAX incorporates a linear regression over each
segment into SAX \mycite{malinowski2013}. A combination of SAX and a polynomial
approximation was used to speed up the SAX method \mycite{fuad2010}. To improve 
the indexing performance of SAX, iSAX introduced convertible alphabet sizes, 
allowing SAX
representations with different alphabet sizes to be compared with each other and
indexed into a tree structure \mycite{shieh2008}. iSAX 2.0 improves the iSAX
index by reducing its computational complexity, enabling it to index a time
series that has one billion elements, something that SAX or iSAX cannot do
\mycite{camerra2010a}. To perform time series anomaly detection using SAX, 
\citeauthor{keogh2005}
introduced Heuristically Ordered Time series using SAX (HOT SAX) in
\citeyear{keogh2005}. Specifically, the authors attempt to detect time series
``discords", a subsequence of a time series that is most different from other
segments of the time series. This can theoretically be done by simply
comparing all subsequences of the raw time series to all other segments, but
this approach is not feasible for long time series because of its complexity. 
Thus, HOT SAX utilizes SAX to reduce the dimensionality and complexity of the
time series and then sorts the resulting SAX segments to speed up the discord
detection. The authors suggest further research to investigate the use HOT SAX
on multivariate time series \mycite{keogh2005}. For an in-depth description of 
this method, please refer to section \ref{08:hot-sax}.\par

SAX and its variants have also been used for the analysis of multivariate time
series. SAX-ARM combines the SAX representation with association rule mining 
(identifying rules and implications
found in the data, i.e. parameter a influences parameter b) to analyze
multivariate time series and discover the rules underlying the data
\mycite{park2020}. \citeauthor{anacleto2020} introduced MSAX in
\citeyear{anacleto2020} and thus expanded the use of SAX to multivariate time 
series. They utilize multivariate normalization with a covariance matrix and a 
modified distance measure to achieve this. To analyze their method, the authors
use MSAX and SAX in a classification task based on multiple multivariate time
series datasets. For these multivariate datasets, SAX was applied to each of
the time series and those results were combined. Their analysis 
found that, overall, SAX applied in this way is superior to MSAX when it comes
to classification accuracy. In 6 of the 14 tested datasets, SAX was
significantly more accurate, in 2 of the MSAX was more accurate, and in the
remaining 6 their performance was not significantly different. It should be
noted that in the ECG dataset they tested, the accuracy of SAX
($\sim$87\%) was slightly higher than that of MSAX ($\sim$84\%), but not significantly so. \citeauthor{anacleto2020}
suggest that in future research MSAX should be applied to electronic health
records (e.g. ECGs) and that it should be applied to other time series data
mining applications besides classification \mycite{anacleto2020}. MSAX will be
thoroughly presented in section \ref{08:msax}.
Another application of SAX to multivariate data used it to visualize multivariate 
medical test results and enable their analysis \mycite{ordonez2008}.
Resource-aware SAX is a SAX variant developed to analyze ECG using a mobile
device like a mobile phone. The method takes advantage of the computational
efficiency of SAX to perform the ECG analysis on the device and even preserve
its battery life. Another application of the SAX method to ECGs is
\mycite{zhang2019}, which uses SAX with an added binary measure of the trend of
each segment to detect ECG anomalies, achieving a recall value of 98\%. The
section \ref{07:section_ecg} below will elaborate on ECGs and methods of their
analysis. 

\subsection{ECGs and ECG Analysis}\label{07:section_ecg}

The following subsection covers the ECG and methods used in its analysis.
Luigi Galvani noted the electrical activity in muscles in 1786, but the history of 
the ECG only started in 1842, when Carlo Matteucci showed the electrical 
activity of a frog's heartbeat. In the 1870s, it was discovered that each
heartbeat is characterized by electrical changes.   
Then, in 1901-1902, Willem Einthoven created the first ECG recording of a human 
heartbeat using 3 leads connected to the limbs of the patient.
Einthoven was the
first to publish an ECG waveform with the now-standard annotations P, Q, R, S,
and T for the different features (see \figref{07:fig:ecg_ann}).
He would receive the 1924 Nobel Prize in medicine for his invention of
the electrocardiograph. As a result of further development, the 12-lead ECG that 
we know today was created \mycite{alghatrif2012,fye1994}. The 12-lead ECG is
comprised of 6 chest leads (measurements of electrodes on the chest) numbered
consecutively V1 to V6, as well as 6 limb leads (measurements of electrodes on 
the limbs) called I, II, III, aVR, aVL, aVF \mycite{meek2002}.\par

\subsubsection{What is an ECG?}\label{07:what-is-an-ecg}

An ECG records the electrical activity that accompanies the contraction and
relaxation of the heart muscle. The sinuatrial node, which can spontaneously
give off an electrical pulse, initiates the heartbeat. Its pulse is conducted
through the heart by other specialized fibers, causing the heart to beat. The
conduction of electricity is facilitated by Sodium, Calcium, and Potassium ions
flowing in and out of cardiac cells \mycite{becker2006}. 
\figref{07:fig:ecg_ann} shows an ECG wave of a single heartbeat from record 103
of the MIT-BIH database \mycite{moody2001,goldberger2000} (for more 
information on the database, see section \ref{07:ecg-db}).
The P wave is caused by the depolarization of the atrial node, which allows
blood to flow into the heart. The QRS complex, as it is called, is the result
of ventricular depolarization and represents the action of pumping blood out of
the heart. The T wave is caused by ventricular repolarization in preparation
for the next heartbeat. The U wave, only present in about 25\% of people, is
thought to he caused by mechanical-electric feedback 
\mycite{wasilewski2012,becker2006}. The last P wave is part of the next 
heartbeat, which is not shown in \figref{07:fig:ecg_ann}. 
\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth,height=0.7\textwidth]{07_ecg_ann}%
    \caption{Annotated ECG of one heartbeat. This graph is based on lead II, 
    data points 2031--2390 of recording 103 of the MIT-BIH database 
    \mycite{moody2001,goldberger2000}.}
    \label{07:fig:ecg_ann}
\end{figure}

The waves and complexes shown in \figref{07:fig:ecg_ann} are the object of ECG
analysis. Changes in their shape, duration, or height can indicate heart
conditions. \citeauthor{becker2006} list some of the features relevant for ECG
analysis \mycite{becker2006}:
\begin{itemize}
    \item The regularity of the rhythm: are the intervals between the QRS
        complexes and P waves regular?
    \item The shape of the QRS complex: do they have similar shape and
        duration?
    \item The regularity of the P waves: are the P waves similar and is the
        interval between P wave and QRS complex similar?
    \item Is the heart rate regular: measuring the time between QRS complexes
        can be used to calculate the heart rate, is this heart rate in the normal
        range?
    \item Do the waves and complexes come in the same order each time: each
        cycle should consist of a P wave, QRS complex, T wave.
\end{itemize}
Using an ECG to diagnose a cardiac condition is difficult in practice. Small 
changes in the components of the ECG can be indicators of diseases and those 
changes can be overlooked, even by trained and specialized physicians. The 
chance to make a mistake is even higher for non-specialized physicians and 
trainees \mycite{alghatrif2012,xie2020}. The American Heart Association (AHA)
estimates that a physician needs to read at least 500 ECGs with the help of an
expert before becoming proficient. One reason for this is that the number of 
diagnoses that can be performed using an ECG is vast. The AHA lists 88 
different conditions and an additional 22 diagnoses related to diseases and 
conditions that may not directly affect the heart, such as hypothermia or 
tremors caused by Parkinson's disease \mycite{kadish2001}.\par

Two types of heart conditions that an ECG can detect are cardiac arrhythmias
and ischemic heart disease. Cardiac arrhythmia is a variation of the heart
rate or rhythm that does not have a reasonable cause. In other words, heart
rate or rhythm variations caused by physical activity could not be considered 
arrhythmias, while significant variations in a resting state may
\mycite{antzelevitch2011}. In an ECG, arrhythmia is most apparent in changes in
the interval between the QRS complexes. Ischemic heart disease is the main 
cause of death worldwide \mycite{nowbar2019}. Ischemic heart disease is 
characterized by 
restricted blood flow to an area of the heart, causing it to not receive 
enough blood and oxygen, as mentioned in the introduction. In an ECG, ischemic 
heart disease can be
diagnosed based on changes in the ST segment and the T wave. The diagnosis of 
ischemic heart disease and
other heart diseases is time sensitive. If a patient has suffers from a heart 
attack, treatment has to be started as soon as possible. Some forms of 
treatment are most effective in the first 3 hours after symptom onset and lose 
most of their effectiveness after 9 to 12 hours. The diagnosis required for 
treatment to begin should thus be as quick as possible. The real-time
information delivery of an ECG is an advantage in this situation, even though 
there are more time-consuming methods that can deliver more accurate results 
than an ECG \mycite{herring2006}.\par

\subsubsection{Computerized ECG analysis}

The widespread use of ECGs and the time-sensitive nature of their application 
as diagnostic tools make errors, delays, or inconsistencies in their 
interpretation unacceptable. A recent approach to minimizing this problem is 
the application of computer technology in ECG recording, storage, and analysis
\mycite{kligfield2007}. Time series analysis methods can also be applied to
ECGs because ECGs simply represent discrete multivariate time series. As
discussed in section \ref{07:time-series-intro}, multivariate time series are
time series that contain more than one value at each point in time, while
discrete time series are time series that are measured at discrete points in
time or at set intervals. ECGs fulfill both of these requirements, as all
modern ECGs contain at least 2 leads, most of them 12, and they have set
sampling rates, given in samples per second. The common steps of computerized 
ECG analysis, following \mycite{kligfield2007}, are:
\begin{enumerate}
    \item \label{07:comp-ecg-an:01} signal acquisition and filtering,
    \item \label{07:comp-ecg-an:02} data transformation,
    \item \label{07:comp-ecg-an:03} waveform recognition,
    \item \label{07:comp-ecg-an:04} feature extraction, and
    \item \label{07:comp-ecg-an:05} classification or diagnosis.
\end{enumerate}

Step \ref{07:comp-ecg-an:01} comprises the digital recording of ECG signals or
the digitizing of paper-based ECG records. For either process, the AHA
recommends a sampling frequency of 500 samples per second. ECG filtering is 
performed to remove noise introduced by patient movements, power line 
interference, and other factors \mycite{kligfield2007}. This filtering, or
denoising, is often performed using digital filters. Their drawbacks are that 
they only filter out very specific frequencies. Because noisy ECGs contain 
different types of contaminations, digital filters can be inaccurate. Using 
wavelet transforms for denoising has the advantage that noise can be more 
precisely targeted and the clean signal reconstructed afterward. Choosing 
appropriate wavelet parameters can be challenging, but methods to optimize this 
process have been proposed \mycite{xie2020}. Step \ref{07:comp-ecg-an:02} uses
the same types of methods for dimension reduction that were discussed in 
the above sections for 
time series and shall not be repeated here. This includes SAX, which has been
successfully applied to ECG analysis \mycite{zhang2019}. MSAX has, at the time
of writing, to the author's knowledge not been applied to ECG analysis. HOT SAX
has been used in \mycite{sivaraks2015} to detect anomalies in ECGs. It was
found to detect anomalies, but it exhibited a larger amount of false
identifications than competing methods.\par

Steps \ref{07:comp-ecg-an:03} and \ref{07:comp-ecg-an:04}, the waveform recognition and feature extraction
steps, are signified by extracting features that are relevant for diagnosis
from the many points of the ECG. This process can also be aided by an
appropriate representation chosen in the previous step. 
The main features targeted for extraction are the PQRST 
features shown previously in \figref{07:fig:ecg_ann}. The Fast Fourier 
Transform provides a way of 
analysing the frequency domain of the ECG signal, enabling the detection of 
the QRS complex \mycite{prasad2018} and other features \mycite{valupadasu2012}. 
The missing time 
information in the Fast Fourier Transform can lead to difficulties in detecting 
time-dependent features. The short-time Fourier Transform adds time information 
to the Fast Fourier Transform's data. This can increase the accuracy of the 
feature extraction. It has the drawback in the tradeoff between the time and 
frequency resolutions. Wavelet transforms can also be used for feature 
extraction \mycite{kaur2016}. They have the advantage that they are suitable for all frequency 
ranges. Choosing the right wavelet base for the desired application can be a 
challenge. The discrete wavelet transform is the most widely used wavelet 
transform, thanks to its computational efficiency. Statistical methods are also 
used to extract features from ECGs; those methods are generally less affected 
by noise in the signal \mycite{xie2020}.\par

After the features of the ECG have been extracted, it is often necessary to 
further reduce the number of features. The reason for this is that a large 
number of features, despite the high accuracy their analysis may yield, require 
a high amount of computation to classify. This lengthy computation can negate 
the advantages gained by high accuracy. Feature reduction sacrifices a certain 
amount of information and sometimes precision, but significantly speeds up the 
classification. There are two approaches to achieve this. First is feature 
selection, a process that attempts to select a subset of the original data that 
adequately describes the whole data. Feature selection can be performed by a 
filter that filters out unnecessary attributes based on some metric. This 
method is relatively simple, but the filtering process removes data and thus 
negatively impacts the precision of further steps. The second method, feature 
extraction, on the other, hand uses dimension reduction methods to keep as 
much of the original information as possible. Principal component analysis 
preserves as much of the variance in the original data as it can
\mycite{kaur2016}. Other 
algorithms focus on separating classes of data, pattern recognition, or 
retaining the structure of the original data \mycite{xie2020}. Here, again, 
the time series representation methods discussed in sections above can be
applied.\par

Finally, the extracted features can be classified; this is stage 
\ref{07:comp-ecg-an:05}. In this stage, judgments are made based on the 
prepared input data and the result should be a disease diagnosis.
Traditionally, this process is performed by a trained professional, as
discussed in section \ref{07:what-is-an-ecg}. In the early stages of 
computerized ECG analysis, classification was performed by algorithms based on
human actions when reading an ECG. Those algorithms were basic and not 
particularly accurate. Currently, the classification at the end of the 
preparation process is performed by a machine learning algorithm. Such models 
include the k-nearest-neighbors model which classifies points 
but which is very expensive to calculate for high-dimensional data. Support 
vector machines are used for pattern recognition and can work with 
small samples. Artificial neural networks are robust and can work with complex 
problems, they are generally more accurate than support vector machines 
\mycite{prasad2018}. The 
newest approach is to forego the stages discussed here and use a single neural
network to perform all the required tasks ``end-to-end". These networks are 
fed raw data and perform the analysis steps internally, as a 
single model \mycite{xie2020}. This approach is relatively new and still
actively researched. The previous approach, too, is enjoying active research
attention. 

\subsubsection{ECG databases}\label{07:ecg-db}

A very important element of computerized ECG analysis is the training data.
This data is used to train algorithms like neural networks, to manually tweak
parameters of methods like SAX, or to validate and test prepared models. To
fulfill these criteria, the data must be freely available to other researchers
to replicate experiments and it should be fully annotated, meaning that experts
determined the diseases that are or are not present as well as annotated the
individual heartbeats. ECG databases fulfill these requirements. One of the
largest repositories of ECG data and physiological data is PhysioNet. 
PhysioNet was founded in 1999 by the National Institutes of Health (USA) and 
offers large collections of freely accessible ECG data \mycite{goldberger2000}.
These datasets vary in their size from around 10 recordings \mycite{baim1986} 
to over 100 \mycite{laguna1997}. The QT Database \mycite{laguna1997} 
(available at \url{https://physionet.org/content/qtdb/1.0.0/}) has annotations 
for all types
of ECG waves (P, QRS, T, and U; see \figref{07:fig:ecg_ann}) for 105 two-lead 
recordings, each 15 minutes long. This database focuses on wave and feature
detection as most ECG datasets only have the QRS complex annotated. The St 
Petersburg INCART 12-lead Arrhythmia Database (available at 
\url{https://physionet.org/content/incartdb/1.0.0/}) contains 75 30-minute 
recordings that contain all 12 ECG leads. The significance of this database is
that it contains all 12 ECG leads, while most ECG databases only contain
2 (see \mycite{laguna1997,moody2001})--this makes it possible to test
multivariate detection methods as well as realistic circumstances, where a raw
ECG would most likely contain 12 leads. The European ST-T Database 
\mycite{taddei1992} (available at 
\url{https://physionet.org/content/edb/1.0.0/}) contains 90 recordings of 79
subjects, each being 2 hours long and containing two leads. This database is
focused on the ST segment and the T wave (hence the name) and thus focuses on
ischemia detection. One of the most used databases in the literature is the 
MIT-BIH (Massachusetts Institute of Technology-Beth Israel
Hospital) Arrhythmia Database (see
\mycite{prasad2018,kanani2020,kaur2016,nygaard1998,sivaraks2015, 
valupadasu2012,zhang2019}). This database is focused on arrhythmia detection
and contains 48 two-lead recordings that are each 30 minutes long.

\end{document}
