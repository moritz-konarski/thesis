\documentclass[../01_main.tex]{subfiles}

\begin{document}

\section{Methods}

This section details the methods used in this work to investigate its
research question and test its hypothesis. Below, a short introduction to time 
series will be given. Following that, the SAX representation will be discussed, 
followed by the MSAX representation, and then the HOT SAX algorithm. The ECG
data used to experimentally test this work's hypothesis is discussed, followed
by a note on the implementation of the methods, and finally the statistical 
methods used for ECG analysis.\par

While MSAX and SAX both are time series representation methods, they can be
applied to ECGs, as EGCs are discrete multivariate time series. Mathematically,
a discrete time series is a series of $T$ observations made at discrete points 
in time, with $n$ values recorded at each moment in time. Following 
\mycite{anacleto2020}, 
\begin{equation}\label{08:eq:ts}
    \left\{ \mathbf{v} [t] \right\}_{t \in \{1, \ldots, T\}}
\end{equation}
is a $n$-variate time series where for each time point $t$,
\begin{equation}\label{08:eq:ts-point}
    \mathbf{v} [t] = \left(v_1[t], \ldots, v_n[t] \right)^T
\end{equation}
represents the values of the time series. If the time series has $n=1$ values
at each time point, it is called univariate, if $n>1$, it is called
multivariate.
For ECGs, the discrete points in 
time are dictated by the sampling frequency, which is the number of
observations made in one second. The number of leads in an ECG is equivalent to
the variable $n$ in \eqref{08:eq:ts-point}. As virtually all ECGs consist of
more than one lead ($n>1$), ECGs are multivariate time series.

\subsection{SAX}\label{08:sax}

The Symbolic Aggregate Approximation, introduced in \citeyear{lin2003} by 
\citeauthor{lin2003}, is a symbolic time series representation 
\mycite{lin2003}. Its main features are the symbolic representation and
dimension reduction of time series data, and the lower bounding of the
Euclidean Distance. A lower bound (or infimum) in set theory is a value that is 
the largest element in a set $S$ that is smaller than all elements in a certain 
subset of $S$. For SAX, lower bounding the Euclidean Distance can be understood
as stating that the SAX distance between two SAX representations is guaranteed
to be smaller than or equal to the ``true" or Euclidean Distance between the
original time series. Accordingly, the distance between two SAX representations
is guaranteed to be representative of the Euclidean Distance between the raw
time series. This feature sets SAX apart from other symbolic time series
representations, and, together with its wide use in the literature 
(see \mycite{aremu2019,fuad2010,guigou2017,he2020,kulahcioglu2021,lin2003,liu2018, 
lkhagva2006,malinowski2013,ordonez2008,pham2010,tayebi2011,zan2016,keogh2005})
and application to ECGs (see \mycite{zhang2019}), makes SAX a promising method 
to use.
The SAX representation only works for time series $\mathbf{v}[t]$
for which $n=1$, i.e. which are univariate. Thus \eqref{08:eq:ts} becomes
$\mathbf{v}[t] = v_1[t]$. Applying
the SAX representation is a three-step process. Firstly, the raw time series is
normalized. Secondly, the dimension of the normalized time series is reduced 
using the Piecewise Aggregate Approximation (PAA). Thirdly, the PAA-represented 
time series is discretized. Additionally, a distance measure between two
equal-length SAX representations is defined.

\paragraph{Normalization.}

The normalization for the SAX representation is necessary because, to compare
two time series, it is standard practice to normalize both of them because
otherwise, comparisons between them are not useful \mycite{lin2003}. SAX is
normalized by applying standard Z-normalization, resulting in a time series
with sample mean equal to 0 and sample standard deviation equal to 1. To do 
this, the mean and standard deviation of the univariate time series 
$\mathbf{v}[t]$ needs to be calculated. The sample mean of a list of values is
\begin{equation}\nonumber
    \overline{x} = \frac{1}{T} \sum_{t=1}^T{\mathbf{v}[t]}.
\end{equation}
The sample standard deviation can be found with the formula
\begin{equation}\nonumber
    s = \sqrt{\frac{1}{T-1} \sum_{t=1}^T{\left(\mathbf{v}[t] - \overline{x} \right)^2}}
\end{equation}
(It should be noted that for applications to whole ECGs, the sample standard
deviation and population standard deviation are very similar, as $T$ is often
$>100.000$). Finally, the normalized time series values can be obtained by
computing
\begin{equation}\nonumber
    \mathbf{v}[t] = \frac{\mathbf{v}[t] - \overline{x}}{s}, \qquad \forall t \in 
    \{1,\ldots,T\}.
\end{equation}
The resulting time series will have the same shape as the raw time series, but
it will have no unit and be normalized.

\paragraph{Dimension reduction with PAA.}\label{08:paa-proc}

The dimension reduction of the SAX representation is due to the use of PAA. The
PAA method takes a univariate time series $\mathbf{v}[t]$ of length $T$ and an
integer $w$ and segments $\mathbf{v}[t]$ into $w$ segments, taking the average
of each. Following \mycite{lin2003}, the resulting representation is denoted as
$\overline{\mathbf{v}}[t]$ and now has length $w$. The PAA representation of
$\mathbf{v}[t]$ can be calculated by using the following formula 
\mycite{lin2003}
\begin{equation}\label{08:eq:paa}
    \overline{\mathbf{v}}[t] = \frac{w}{T} \sum_{j
    = \frac{n}{w}(t-1)+1}^{\frac{n}{w}t}{\mathbf{v}[t]},\qquad \forall t \in
    \{1,\ldots,w\}.
\end{equation}
Now $\mathbf{v}[t]$ has been converted to the PAA representation
$\overline{\mathbf{v}}[t]$.
This process reduces the length of the time series from $T$ to $w$, with the
dimension reduction ratio depending on the choice of $w$. In
\figref{08:fig:paa}, the PAA representation is shown overlaid onto ECG 103 of
the MIT-BIH database. It is apparent that the dimension of the ECG has been
reduced. The original ECG section of one second contains 360 data points, while
its PAA representation only contains 18 values--a dimension reduction of 20.
The parameter $w$ is one of only two parameters that determine the SAX
representation. A low number of parameters can be a great advantage in time
series analysis because it is simpler to optimize a few, understandable
parameters compared to many unintuitive parameters.
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{paa_graph}
    \caption{Graph of ECG 103 of the MIT-BIH database overlaid with its PAA
    representation. Here $w=18$, meaning there are 18 PAA segments. The
    dimension reduction through PAA is 20.}
    \label{08:fig:paa}
\end{figure}

\paragraph{Discretization of PAA representation.}

This last step in the SAX representation process involves transforming the PAA
representation $\overline{\mathbf{v}}[t]$ into a sequence of equiprobable symbols. Here it is assumed that
a normalized time series has a Gaussian normal distribution ($\mathcal{N}(0,
1)$). The number symbols used is denoted by $a$--the alphabet size. To create 
the equiprobable 
symbols, \textcite{lin2003} use so-called ``breakpoints". These breakpoints are 
a sorted list of numbers $B = \beta_1,\ldots,\beta_{a-1}$. The area under the
normal curve $\mathcal{N}(0,1)$ (i.e. the probability) between two consecutive
segments $\beta_i$ and $\beta_{i+1}=1/a$. This creates $a$ segments ($a-1$
breakpoints) of $\mathcal{N}(0,1)$ that have the same area, i.e. the same 
probability. The values of the breakpoints in $B$ can be found in a Z-table. 
For illustration, \tabref{08:tab:B} shows the breakpoint values for $a=3$ to 
$a=6$. This parameter is the second and last parameter that influences the SAX
representation.
\begin{table}[H]
    \centering
    \caption{Breakpoint values for numbers of breakpoints $a$ from 3 to 6. The
    parameter $a$ determines into how many equally-sized areas the normal curve 
    $\mathcal{N}(0,1)$ is split. The breakpoints $\beta_i$ delimit the areas.
    Table contents are quoted from \mycite{lin2003}.}
    \label{08:tab:B}
    \vspace{1em}
    \begin{tabular}{|>{\columncolor{tablegray}}c | r | r | r | r |}\hline
        \rowcolor{tablegray}
        {\centering\diagbox{$\beta_i$}{$a$}}& \multicolumn{1}{c|}{3} & 
        \multicolumn{1}{c|}{4} & \multicolumn{1}{c|}{5}  & 
        \multicolumn{1}{c|}{6}   \\\hline
        $\beta_1$   & -0.43 & -0.67  & -0.84 & -0.97    \\\hline
        $\beta_2$   &  0.43 &   0  & -0.25 & -0.43    \\\hline
        $\beta_3$   & \multicolumn{1}{c|}{---} &  0.67  &  0.25 &   0    \\\hline
        $\beta_4$   & \multicolumn{1}{c|}{---} &\multicolumn{1}{c|}{---}&  
        0.84 &  0.43    \\\hline
        $\beta_5$   & \multicolumn{1}{c|}{---} &\multicolumn{1}{c|}{---}&
        \multicolumn{1}{c|}{---}&  0.97    \\\hline
    \end{tabular}
\end{table}

Once the breakpoint values have been determined, the discretization process
begins. The process assigns all PAA segments whose value is below $\beta_1$
the symbol ``a". The PAA segments falling in the area $\beta_1 \le$ and $<
\beta_2$ are assigned ``b". This mapping process is continued until all PAA
segments are symbolized. Now we have arrived at the SAX representation. The SAX 
representation of $\overline{\mathbf{v}}[t]$ is denoted $\widehat{\mathbf{v}}[t]$ and has 
the same length as $\overline{\mathbf{v}}[t]$ ($w$). Mathematically, the
discretization process is formulated in \mycite{lin2003} as
\begin{equation}\nonumber
    \widehat{ \mathbf{v}}[t] = \text{alpha}_j \quad \text{if  } \beta_{j-1} \le
    \widehat{\mathbf{v}}[t] < \beta_j, \qquad \forall t \in \{1,\ldots,w\}.
\end{equation}
Here $\text{alpha}_j$ is the $j$th letter of the alphabet, i.e.
$\text{alpha}_1=$ ``a", $\text{alpha}_2=$ ``b" \ldots{}
The resulting time series representation has even lower complexity than
PAA because instead of infinitely many possible values for the real-valued PAA 
values, now there are only $a$ different, equiprobable symbols. Thus, the SAX
representation $\widehat{\mathbf{v}}[t]$ has been obtained. In
\figref{08:fig:sax} ECG 103 of the MIT-BIH database is shown with $w=18$ and
$a=4$. The three breakpoints are indicated by the dashed horizontal lines.
\figref{08:fig:sax} illustrates how an ECG can be reduced from 360 real-valued
points to 18 symbols. The SAX representation of this ECG section is
``bbbbb\textbf{d}bbbc\textbf{d}cbddbbb". Both the QRS complex (the first ``d") 
and the T wave (second ``d") can be seen in the representation.
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{sax_graph}
    \caption{Graph of ECG 103 of the MIT-BIH database overlaid with its PAA
    representation and the SAX discretization. Here $w=18$, a dimension
    reduction of 20, and $a=4$, meaning there are 3 breakpoints (indicated by
    the dashed lines) and 4 symbols.}
    \label{08:fig:sax}
\end{figure}

\paragraph{SAX distance measure.}

A distance measure between two SAX representations of the same length is
required to be able to compare them with each other. The SAX distance function
is based on the Euclidean Distance between two time series $\mathbf{v}[t]$ and 
$\mathbf{u}[t]$ is \mycite{lin2003}
\begin{equation}\nonumber
    \text{D}\left(\mathbf{u}[t],\mathbf{v}[t] \right) \equiv \sqrt{\sum_{t=1}^{T}
    {\left(\mathbf{u}[t] - \mathbf{v}[t] \right)^2}}.
\end{equation}
Through the PAA distance as an intermediate step, the authors arrive at
$MINDIST$ in \eqref{08:eq:sax-mindist}, the SAX distance function that 
returns the minimum distance between the two original time series. It is 
defined as \mycite{lin2003}
\begin{equation}\label{08:eq:sax-mindist}
    MINDIST\left(\widehat{\mathbf{u}}[t], \widehat{\mathbf{v}}[t] \right) \equiv 
    \sqrt{\frac{T}{w}}
    \sqrt{\sum_{t=1}^{w}{\left(dist(\widehat{\mathbf{u}}[t], 
    \widehat{\mathbf{v}}[t]) \right)^2}}.
\end{equation}
The function $dist$ is based on a lookup table that contains the
distances between two symbols. \tabref{08:tab:dist} shows the lookup table for
$a=5$. The values of each table cell are 0 for symbols letters or the absolute
difference of the breakpoints otherwise. The formula
\begin{equation}\label{08:eq:dist-cell}
    \text{cell}_{r,c} = 
    \left\{ \, 
        \begin{aligned}
            0, 
                &\quad \text{if}\,\,\, |r-c| \le 1 \\ 
            \beta_{\text{max}(r,c)-1} - \beta_{\text{min}(r,c)}, 
                &\quad \text{otherwise}
        \end{aligned} 
    \right.
\end{equation}
is used to calculate the values of each cell in \tabref{08:tab:dist} by $r$
(row) and $c$ (column) \mycite{lin2003}.
\begin{table}[H]
    \centering
    \caption{A table for the $dist$ function for $a=5$. Each cell 
    displays the distance between the symbols denoting its row and column. The
    formula for the cell values is \eqref{08:eq:dist-cell}.}
    \label{08:tab:dist}
    \vspace{1em}
    \begin{tabular}{|>{\columncolor{tablegray}}c | r | r | r | r | r |}\hline
        \rowcolor{tablegray} & \multicolumn{1}{c|}{a} & \multicolumn{1}{c|}{b} 
            & \multicolumn{1}{c|}{c} & \multicolumn{1}{c|}{d} & 
            \multicolumn{1}{c|}{e} \\\hline
        a & 0       & 0        &0.59     &1.09     &1.68  \\\hline
        b & 0       & 0        &0        &0.51     &1.09  \\\hline
        c & 0.59    & 0        &0        &0        &0.59  \\\hline 
        d & 1.09    & 0.51     &0        &0        &0     \\\hline 
        e & 1.68    & 1.09     &0.59     &0        &0     \\\hline 
    \end{tabular}
\end{table}
For example, if $a=5$, the $dist$ between ``a" and ``a" is 0, just 
like the distance between ``b" and ``a". The distance between ``d" and ``a" is 
1.09.

\subsection{MSAX}\label{08:msax}

The Multivariate Symbolic Aggregate Approximation was introduced by
\citeauthor{anacleto2020} in \citeyear{anacleto2020}. It is an extension of SAX
to multivariate time series \mycite{anacleto2020}. It shares the main features
of SAX but expands them to multivariate time series, such as ECGs--$n$ can be
any integer $\ge1$. A lower 
bound for the MSAX distance function also exists, i.e. distance between two 
MSAX representations is, just as in SAX, guaranteed to be representative of 
the Euclidean Distance between the raw time series. As MSAX builds on the
legacy of SAX and purports to improve upon it, it makes a good research topic.
Further, having only been introduced in \citeyear{anacleto2020}, MSAX is new
and there is still much to be learned about it and its applications. The very
similar performance the authors observed between SAX and MSAX in ECG
applications motivate further research in this area as they note in their
conclusion \mycite{anacleto2020}. Using the MSAX 
representation has the same steps as SAX: normalization, PAA-based dimension
reduction, and discretization. A variation of the $MINDIST$ function
exists, too. Like SAX, MSAX only depends on two parameters, the PAA segment
count $w$ and the alphabet size $a$. This makes it simple to optimize MSAX as
the parameters are few and intuitive.

\paragraph{Normalization.}

The rationale for normalization in the MSAX representation is twofold. Firstly,
the same considerations as for SAX apply with regards to comparing two time
series. Secondly, MSAX utilizes multivariate normalization to take advantage of
the covariance structure of multivariate time series data. To avoid confusion
with the previous section, a multivariate time series shall be denoted as 
$\mathbf{V}[t]$. Multivariate normalization relies on a sample mean vector 
containing the sample mean for each of the time series $\left(V_1[t], \ldots, 
V_n[t] \right)^T$ in $\mathbf{V}[t]$. The sample standard deviation is replaced
by a covariance matrix. The sample mean vector is equivalent to the vector of
expected values $\vec E$, following \mycite{anacleto2020}:
\begin{equation}\nonumber
    E(\mathbf{V}[t]) = \vec E = 
    \begin{bmatrix} 
        \text{mean}(V_1[t]) \\ 
        \vdots \\ 
        \text{mean}(V_n[t]) 
    \end{bmatrix}
    = 
    \begin{bmatrix} 
        \frac{1}{T} \sum_{t=1}^T{V_1[t]} \\
        \vdots \\ 
        \frac{1}{T} \sum_{t=1}^T{V_n[t]} \\
    \end{bmatrix}.
\end{equation}
The covariance matrix, an $n\times n$ matrix, contains the variance of each 
part $\left(V_1[t], \ldots, V_n[t] \right)^T$ of $\mathbf{V}[t]$ on its main 
diagonal, and the covariance
between $i$th and $j$th parts of $\mathbf{V}[t]$ in the $(i,j)$ position. The
general form of a covariance matrix is shown in \eqref{08:eq:cov-mat} below.
The covariance matrix is denoted as $\text{Var}(\mathbf{V}[t])$ or
$\Sigma_{n\times n}$. It is calculated as:
\begin{equation}\label{08:eq:cov-mat}
    \text{Var}(\mathbf{V}[t]) = 
    \Sigma_{n\times n} 
    = 
    \begin{bmatrix}
        \text{cov}(V_1, V_1) & \dots  & \text{cov}(V_1, V_n) \\
        \vdots               & \ddots & \vdots \\
        \text{cov}(V_n, V_1) & \dots  & \text{cov}(V_n, V_n) \\
    \end{bmatrix}.
\end{equation}
The covariance of two time series parts $V_i[t]$ and $V_j[t]$ is defined as the
mean of product of the difference between the values of $V_i[t]$ and its
expected value. The following equation illustrates this process:
\begin{equation}\nonumber
    \text{cov}(V_i[t], V_j[t]) = E\Big( \big[V_i[t] - E(V_i[t])\big] \cdot 
        \big[V_j[t] - E(V_j[t])\big] \Big)
\end{equation}
(Note that $\mathbf{V}[t]$ can be conceptualized as a matrix, with its row
representing the different sub-series and the columns representing specific
values of $t$).
Once $\vec E$ and $\Sigma_{n\times n}$ have been found, the time series 
$\mathbf{V}[t]$ can be normalized by the following formula
\mycite{anacleto2020}:
\begin{equation}\nonumber
    \mathbf{V}[t] = \left(\Sigma_{n\times n}\right)^{-1/2} \left(\mathbf{V}[t]
    - \vec E\right).
\end{equation}
The result will have a mean of zero and uncorrelated variables
\mycite{anacleto2020}.

\paragraph{Dimension reduction with PAA.}

Dimension reduction using PAA for MSAX is performed in the same way as
for SAX. The procedure outlined in the previous section is applied to 
each of the elements $\left(V_1[t], \ldots, V_n[t] \right)^T$ of the time series
$\mathbf{V}[t]$--equation \eqref{08:eq:paa} is applied to each part. This 
results in
a PAA representation of the original time series $\overline{\mathbf{V}}[t] = \left(
\overline{V_1}[t], \ldots, \overline{V_n}[t]\right)^T$. This process also reduces the length 
of the time series from $T$ to $w$ for each sub-series, with the
dimension reduction ratio depending on the choice of $w$ \mycite{anacleto2020}.
The \figref{08:fig:paa}, while showing the PAA representation for SAX, is also
applicable here, as the process is identical for SAX and MSAX.

\paragraph{Discretization of PAA representation.}

The discretization of the PAA representation for MSAX also works like it does
for SAX. Like in the previous paragraph, the process used in the SAX
representation is applied to each of the sub-time series in $\overline{\mathbf{V}}[t]$ to obtain $\widehat{\mathbf{V}}[t]$. The alphabet size $a$ is the
same for each $V_n[t]$ and the symbols are found in the same way as in the SAX
representation. The breakpoint values are calculated the same and
\tabref{08:tab:B} is as valid for MSAX as it is for SAX. The assigning of
symbols is generally performed in the same way, too. For bivariate time series
($n=2$), $V_1[t]$ could be assigned lowercase symbols (``a", ``b" \ldots{})
while $V_2[t]$ could be assigned uppercase symbols (``A", ``B" \ldots{}). This
has no impact on the method, it is simply a visual aid for the viewer to
distinguish the values. The final MSAX representation $\widehat{\mathbf{V}}[t]$ 
will
consist of one long list of symbols because for each moment $t$ all generated
symbols are combined into a list for this time that represent all sub-time
series at that time. \figref{08:fig:msax} shows the MSAX representation of one
second of ECG 100 from the MIT-BIH database. As MSAX is a multivariate
representation, both leads of the ECG are shown. The symbols of the second lead
have been capitalized to distinguish them from the symbols of the first lead.
The parameters for the graph are $w=18$ and $a=4$, leading to a dimension
reduction of 20 and the use of 3 breakpoints (indicated by the dashed lines).
Using the MSAX representation, the two ECG leads, which have 360 data points
each in raw form, can be represented by 36 symbols. The ECG section displayed
below can be expressed as the series of symbols ``cCcCbC\textbf{dD}bCbCbCbBbAbBcCcC cBcBcBcBcBbB". In the MSAX representation, the features of the raw data
are still present, which can be seen in the visible QRS complex indicated by
``dD".
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{msax}
    \caption{Graph of ECG 100 of the MIT-BIH database overlaid with its PAA
    representation and the MSAX discretization, for both leads. Here $w=18$, a 
    dimension reduction of 20, and $a=4$, meaning there are 3 breakpoints 
    (indicated by the dashed lines) and 4 symbols.}
    \label{08:fig:msax}
\end{figure}

\paragraph{MSAX distance measure.}

The MSAX distance measure expands $MINDIST$ to multivariate time series.
This is done by adding an additional summarization step to the $MINDIST$
function. The MSAX distance $MINDIST\_MSAX$ operates on two MSAX 
representations $\widehat{\mathbf{U}}[t],\widehat{\mathbf{V}}[t]$. Both
representations must have the same length $w$ and same number $n$.
$MINDIST\_MSAX$ sums the distances between the individual elements
$U_i[t], V_i[t]$ for $i = \overline{1,\ldots,n}$. The following equations
expresses $MINDIST\_MSAX$ \mycite{anacleto2020}.
\begin{equation}\label{08:eq:msax-mindist}
    MINDIST\_MSAX
        \left(
            \widehat{\mathbf{U}}[t], \widehat{\mathbf{V}}[t] 
        \right) 
    \equiv 
    \sqrt{\frac{T}{w}}
    \sqrt{
        \sum_{t=1}^{w}{
            \left(
                \sum_{i=1}^{n}{
                    \left(
                        dist(\widehat{U_i}[t],\widehat{V_i}[t]) 
                    \right)^2
                }
            \right)
        }
    }.
\end{equation}
The function $dist$ is the same as the SAX function, being based on
equation \eqref{08:eq:dist-cell} and lookup tables like \tabref{08:tab:dist}.
Like $MINDIST$, $MINDIST\_MSAX$ also lower bounds the Euclidean
Distance and derives all the same benefits from that.
The distances between individual MSAX symbols the same as between SAX symbols
(see \tabref{08:tab:dist}). Distances between groups of more than 1 symbol can
be calculated using \eqref{08:eq:msax-mindist}.

\subsection{HOT SAX}\label{08:hot-sax}

The Heuristically Ordered Time series using Symbolic Aggregate Approximation is
a discord discovery algorithm introduced by \citeauthor{keogh2005} in 
\citeyear{keogh2005} \mycite{keogh2005}. Discord discovery is the process of
identifying subsections of a time series that are most different to other
segments of the time series, i.e. that have the largest distance to other,
non-intersecting subsegments \mycite{keogh2005}. The standard approach to
discord discovery, comparing all segments to all other segments, is too slow
for application to large datasets as it has a complexity of $O(m^2)$. This
means that for $m$ subsegments, around $m^2$ operations need to be performed.
This would be performed in two nested loops, the outer loop iterating over all
subsegments. The inner loop also iterates over all subsegments; the subsegments
from the outer and inner loop are compared if and only if they are not
identical. An algorithm for this procedure can be found in Table 1 of
\mycite{keogh2005}. As each of these loops would iterate over all of the $m$ 
subsegments, resulting in the mentioned $m^2$ complexity.\par

HOT SAX has the goal of speeding up this process and making discord discovery
viable even for long time series. The authors theorize that a ``magic"
heuristic would provide the time series subsegments first in order of their
distance to their nearest neighbor, from largest to smallest. These would be
iterated over by the outer loop. Then, the magic heuristic would provide an 
ordering of the subsegments by their distance to the subsegment selected in the
outer loop, in ascending order. Inside the inner loop, the subsegments are them
compared, given that they are not identical. The logic behind the outer and
inner magic heuristics is as follows: the outer heuristic orders the time series
subsegments by their distance to their neighboring segments, descendingly. This
effectively produces a list of subsegments that are most different from the
other segments. Combined with the assumption that time series discords are very 
different from the other segments, the outer heuristic effectively orders the
subsegments by the likelihood that they are discords. The inner heuristic
returns an ordering that produces subsegments with the smallest distances to
the outer-loop subsegment, i.e. it returns the segments most similar to
the outer-loop subsegment. If it is assumed that the outer-loop subsegment is
likely a discord, other subsegments that are similar to it are also likely to
be discords. With these two magic heuristics, discord discovery would be sped
up significantly, as the discords are very likely found early on in the process
and thus the process can be abandoned before exhausting all $m^2$ operations.
Even if the magic heuristic were as bad as possible, returning orderings that
slow down the process as much as possible, the brute force method mentioned in
the previous paragraph would not be faster. In this case, both methods would
require $m^2$ operations and be equal \mycite{keogh2005}.\par

The magic heuristic can of course not exist, hence the name. But
\citeauthor{keogh2005} approximate it to still achieve a significant speedup.
The first step the authors take is to apply the SAX representation to the time
series to reduce its complexity and dimension, while retaining an accurate
representation of the data \mycite{lin2003}. To compare two subsegments, the
SAX distance function $MINDIST$ is used. Then, a certain window size is
chosen that represents the number of SAX segments that will make up one of the
aforementioned time series subsegments. Now the magic heuristic can be
approximated. The outer heuristic, which returns the subsegments of the time
series in descending order by their distance to their neighbors can be
approximated by taking each SAX subsegment and inserting them into an array,
counting how many times each unique time series occurs. This is shown on the
left side of \figref{08:fig:hot-sax}. By sorting this array
by the occurrence counts, the outer heuristic can be approximated. At the same
time as the outer heuristic, the inner heuristic can be approximated. For its
approximation, a digital tree (also known as trie or prefix tree) is used. In
this tree, the SAX representation is used as an index to locate a leaf node.
This node contains the locations in the time series where the particular
subsequence occurs. Effectively, this prefix tree can be used to locate all SAX 
subsequences that are identical to a given one. So, if one has the subsegment 
``abc", it is possible to find all other locations of the ``abc" subsegment in
the time series using this tree. A sketch of such a tree is shown on the left
side of \figref{08:fig:hot-sax}.\par
\begin{figure}[t]
    \centering
    \includegraphics[height=0.4\textheight]{fig4}
    \caption{Illustration of the HOT SAX heuristic creation process. This
    figure illustrates how each unique time series subsegment is recorded in an 
    array together with the count of its occurrence. The augmented trie stores
    the locations of the segments in the time series. The indices can be
    retrived by indexing the trie using a SAX subsequence. This figure is
    quoted from Figure 4 on p. 5 of \mycite{keogh2005}.}
    \label{08:fig:hot-sax}
\end{figure}
By default, HOT SAX only returns the most discordant time series subsegment.
Because more than one time series discord can be present in a time series (as
in an ECG), it is useful to extract more than one. The number of discords to be
extracted is called $k$. If $k>1$, each newly found discord is compared to the
other already found discords, and only the $k$ discords with the largest
distance values are saved. In this way, it is possible to extract an exact
number of discords. Another modification is to save all discords that the
method detects \mycite{keogh2005}.\par

\citeauthor{keogh2005} find that HOT SAX can effectively detect time series 
discords and
speed up the process when compared to the brute force method. A strength of the
method is, in their opinion, the fact that it only requires the user to
determine a single parameter, the length of the subsequence (the parameter $k$
has no influence on the method itself, it simply determines how many of the
results should be used) \mycite{keogh2005}. The authors apply HOT SAX to ECG
time series and, in anecdotal tests, find success in determining discords in
ECGs. They further suggest the application of HOT SAX to multivariate time
series \mycite{keogh2005}. For these reasons, HOT SAX was chosen as this 
research's discord discovery method.\par

While HOT SAX was designed to work with
SAX, all it requires is a time series representation and a lower-bounding
distance measure defined on it. MSAX exhibits both of those traits and can thus
be used with HOT SAX. The resulting algorithm is called HOT MSAX. This
algorithm expands HOT SAX to multivariate time series,
as MSAX can represent those. Using HOT MSAX is novel and a contribution of 
this research.

\paragraph{Limitations of the Methods}

The limitations of the methods used in this research are the following: HOT 
SAX and HOT MSAX are not classifiers based on medically relevant information, 
they classify ECG segments into discords non-discords. The methods to not work
with beat types. This means that their applications to diagnosing heart 
conditions are limited. HOT SAX and HOT MSAX can be used to pre-select specific 
ECG segments to look analyze because they exhibit discords, but it cannot, by 
itself, perform any type of diagnosis. 

\subsection{Data}

The ECG data used in this research to evaluate the HOT SAX and HOT MSAX
algorithms is the MIT-BIH Arrhythmia Database \mycite{moody2001,goldberger2000}. 
This database contains 48 ECG recordings
that are each 30 minutes long. For each of the ECGs, the database contains two
ECG leads and is thus multivariate data. The leads chosen are not the same in
each ECG, they were chosen based on which of the 12 originally recorded ones
best represent the condition of the ECG. A team of cardiologists annotated each
heartbeat in each ECG and determined if it is a normal heartbeat or not. For
example, a beat with the annotation ``N" is a normal beat, while ``A" denotes
an atrial premature beat. The annotations also include non-beat features such
as a change in signal quality, denoted by ``$\sim$", or a rhythm change, which
is denoted as ``+". A full list of annotations and their meaning is available
at \url{https://archive.physionet.org/physiobank/annotations.shtml}. These
annotations make it possible to judge the performance of a discord detection
algorithm, as each detected discord can be checked for correctness using the
provided annotations. While the MIT-BIH database is not the only database that
possesses such annotations, it is one of the most commonly used ones in the
literature (see \mycite{prasad2018,kanani2020,kaur2016,nygaard1998,sivaraks2015,
valupadasu2012,zhang2019}) and it represents a middle ground in a couple of
important respects. The databases 48 ECGs are a manageable number, falling in
between the extremes of around 10 and over 100 ECGs. Furthermore, the 30
minutes length represent real-world ECGs better than 10-second excerpts but
are not as long and analysis-intensive as 24-hour recordings. Lastly, the
MIT-BIH database has a sampling frequency of 360 samples per second, which is
an adequate value \mycite{kligfield2007}. The ECG data in this database is
unfiltered, which adds to the real-world nature of it, as every recorded ECG
contains noise of varying levels.\par

\subsection{Implementation}

The main program for this research was developed using the Julia programming    
language. Julia is a scientific programming language that has similarities to   
R, MATLAB, and Python. Julia possesses a rich ecosystem of libraries for        
visualization, computation, and data manipulation. For more information, visit  
the Julia website at \url{https://julialang.org/}. The Julia program performs
the SAX and MSAX transformation as well as the HOT SAX and HOT MSAX discord
discovery and saves the result to a file for further processing. During the SAX
and MSAX transformations, the parameters $w$ and $a$ define the behavior of the
representations. When performing HOT SAX and HOT MSAX discord detection, the
parameter $m$, the subsequence length, is considered. The parameter $k$ does
not influence the actual behavior of HOT SAX or HOT MSAX, but it influences the
results by controlling the number of detected discords that are returned by the
methods. For each analyzed parameter set, the results and parameter values will
be saved for further analysis.\par

The statistical analysis was performed using the R programming language, a 
well-known statistical programming language.\par

All the code used in this work is available upon request
(please contact \Verb+konarski_m@auca.kg+).

\paragraph{Limitations of the Implementation}

The implementation of the SAX and MSAX representations only allows the use of 
PAA segment counts $w$ that are factors of the sampling frequency of the ECG 
database. This was done so that the whole
ECG, being an even multiple of the sampling frequency itself, can be evenly
divided into PAA segments. This decision prohibits certain numbers of PAA
segments as there may numbers that are not factors of the sampling frequency
but that do evenly divide the number of raw data points in the ECG.
A further simplification step in the same vein is the restriction of
subsequence values $m$ to numbers that evenly divide the number of PAA segments
$w$. This was also done to simplify the process and to guarantee that the whole
ECG would be evenly divisible into subsequences.

\subsection{Statistical Analysis Methods}\label{08:stat-analysis}

After completing the computations of HOT SAX and HOT MSAX for different sets of 
parameters, the results need to be analyzed. Neither HOT SAX nor HOT MSAX are 
classifiers in the sense of classifying heartbeats by medical standards, they
do classify them into discords and non-discords. Thus, they represent binary 
classifiers. Binary classifiers can be evaluated using the well-known True 
Positive, True Negative, False Negative, and False Positive values. 
\tabref{08:tab:bin-class} shows their relationship.
\begin{table}[t]
    \centering
    \caption{Contingency table showing the relationship between detected
    discords and actual annotated values.}
    \label{08:tab:bin-class}
    \vspace{1em}
    \begin{tabular}{|>{\columncolor{tablegray}}c | c | c |}\hline
        \rowcolor{tablegray}
        {\centering\diagbox{Actual}{Assigned}}& Discord Detected & Non-Discord
        Detected \\\hline
        Is Discord      & True Positive     & False Negative \\\hline
        Is Non-Discord  & False Positive    & True Negative  \\\hline
    \end{tabular}
\end{table}
The values in \tabref{08:tab:bin-class} can be used to calculate many useful
ratios that assist the evaluation of the HOT SAX and HOT MSAX algorithms. This 
research uses the recall value (also known as sensitivity), the accuracy, and 
the precision. These ratios are calculated as follows: recall value is defined 
as
\begin{equation}\nonumber
    \text{Recall} = \frac{\text{True Positive}}{\text{True Positive}
    + \text{False Negative}},
\end{equation}
the accuracy as
\begin{equation}\nonumber
    \text{Accuracy} = \frac{\text{True Positive} + \text{True Negative}}
        {\text{True Positive} + \text{True Negative} + \text{False Positive} +
        \text{False Negative}},
\end{equation}
and the precision as
\begin{equation}\nonumber
    \text{Precision} = \frac{\text{True Positive}}{\text{True Positive}
    + \text{False Positive}}.
\end{equation}
Recall can be understood as a measure of how many of the actual discords were
correctly assigned the label discord. This is the most important measure for the
analysis of HOT SAX and HOT MSAX applied to ECGs because in a medical scenario, 
identifying
as many possibly relevant sections of the ECG is more important than being
100\% accurate in their identification. The second most important value is
precision, which can be understood as a measure of how many of the detected
discords are actually discords. While it is more important to identify as many
discords as possible, a 100\% recall rate could be achieved simply by assigning
the label of discord to every element in the time series. Furthermore,
detecting too many non-discords as discords makes it harder to analyze the
actual discords that were highlighted. This of course is not useful, and thus
the precision of HOT SAX needs to be incorporated into the analysis. Lastly,
accuracy is not a very good measure for this particular application, as the
majority of the segments in an ECG are non-discords and HOT SAX/MSAX only 
detects a minority of the segments in an ECG. This leads to a high True 
Negative rate and thus a relatively high accuracy, even if HOT SAX/MSAX did not 
detect any actual discords. Nonetheless, accuracy is a very common indicator of
classifier performance and is calculated here.\par

Once these statistical measures have been calculated, the parameter
optimization can begin. During this process, the large set of parameters will
be reduced until an optimal set of parameters $k$, $w$, $m$, and $a$ is found.
The first step in this process is the introduction of a threshold. As stated
above, recall is the most important statistical factor in this work.
Accordingly, only parameter sets that achieve an average recall value of 95\% 
or above for the MIT-BIH database. Only the subset of the data fulfilling this
requirement will be considered. As the second most important parameter,
precision is considered next. The subset of the data equal to or greater than
the threshold will be sorted by average precision in descending order, singling 
out the parameter sets that have a recall value $\ge$ 95\% and, in this subset, 
the highest precision. Next, the first 10 parameter sets will be chosen from this
subset. Those represent a kind of ``top 10" of the parameters: they fulfill the
threshold and have the highest precision values. Out of these top 10 values,
one optimal set of parameters should be chosen. This should be possible by
creating a box plot of the 10 ten parameter sets vs their recall value. If the
visual analysis does not yield concrete results, the method with the lowest
spread (standard deviation or interquartile range) should be considered the
optimal method. If this should prove inconclusive as well, an analysis of
possible outliers, the distribution of precision values, or the distribution of
the accuracy could be performed.

\end{document}
