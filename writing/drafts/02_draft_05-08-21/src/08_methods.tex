\documentclass[../01_main.tex]{subfiles}

\begin{document}

\section{Methods}

This section details the methods used in this paper to investigate its
hypothesis: does the use of the MSAX representation improve the performance of
the HOT SAX anomaly detection algorithm applied to ECGs compared to the SAX
representation? \TODO{fix this and make congruent with hypothesis}. The
following section will first cover the mathematical foundations of SAX, MSAX,
and HOT SAX. Then, the statistical methods used to analyze the results will be
presented, followed by a note on the implementation of the mathematical methods
and statistical analysis, and the data used in this research. Lastly, the 
limitations of these methods will be discussed. \TODO{make sure this fits the 
actual structure}\par

While MSAX and SAX both are time series representation methods, they can be
applied to ECGs, as EGCs are discrete multivariate time series. Mathematically,
a discrete time series is a series of $T$ observations made at discrete points 
in time, with $n$ values recorded at each moment in time. Following 
\mycite{anacleto2020}, 
\begin{equation}\label{08:eq:ts}
    \left\{ \mathbf{v} [t] \right\}_{t \in \{1, \ldots, T\}}
\end{equation}
is a $n$-variate time series where, for each time point $t$,
\begin{equation}\label{08:eq:ts-point}
    \mathbf{v} [t] = \left(v_1[t], \ldots, v_n[t] \right)^T
\end{equation}
represents the values of the time series. If the time series has $n=1$ values
at each time point, it is called univariate, if $n>1$, it is called
multivariate.
For ECGs, the discrete points in 
time are dictated by the sampling frequency, which is the number of
observations made in one second. The number of leads in an ECG is equivalent to
the variable $n$ in \eqref{08:eq:ts-point}. As virtually all ECGs consist of
more than one lead ($n>1$), ECGs are multivariate time series.

\subsection{Mathematical Foundations}

The main method used in this research is the SAX representation. It will be
compared to MSAX, a multivariate version of the representation. Both
representations will be used with the HOT SAX algorithm to detect ECG
anomalies.
\TODO{make sure this is accurate}

\subsubsection{SAX}

The Symbolic Aggregate Approximation, introduced in \citeyear{lin2003} by 
\citeauthor{lin2003}, is a symbolic time series representation 
\mycite{lin2003}. Its main features are the symbolic representation and
dimension reduction of time series data, and the lower bounding of the
Euclidean Distance. A lower bound (or infimum) in set theory is a value that is 
the largest element in a set $S$ that is smaller than all elements in a certain 
subset of $S$. For SAX, lower bounding the Euclidean Distance can be understood
as stating that the SAX distance between two SAX representations is guaranteed
to be smaller than or equal to the ``true" or Euclidean Distance between the
original time series. Accordingly, the distance between two SAX representations
is guaranteed to be representative of the Euclidean Distance between the raw
time series. This feature sets SAX apart from other symbolic time series
representations, and, together with its wide use in the literature 
\mycite{aremu2019,fuad2010,guigou2017,he2020,kulahcioglu2021,lin2003,liu2018, 
lkhagva2006,malinowski2013,ordonez2008,pham2010,tayebi2011,zan2016,keogh2005}
and application to ECGs \mycite{zhang2019}, makes SAX a promising methods to use.
The SAX representation only works for time series $\mathbf{v}[t]$
for which $n=1$, i.e. which are univariate. Thus \eqref{08:eq:ts} becomes
$\mathbf{v}[t] = v_1[t]$. Using
the SAX representation is a three-step process. Firstly, the raw time series is
normalized. Secondly, the dimension of the normalized time series is reduced 
using PAA. Thirdly, the PAA-represented time series is discretized.
Additionally, a distance measure between two SAX representations is defined.

\paragraph{Normalization.}

The normalization for the SAX representation is necessary because, to compare
two time series, it is standard practice to normalize both of them because
otherwise comparisons between them are not useful \mycite{lin2003}. SAX is
normalized by applying standard Z-normalization, resulting in a time series
with sample mean equal to 0 and sample standard deviation equal to 1. To do 
this, the mean and standard deviation of the univariate time series 
$\mathbf{v}[t]$ needs to be calculated. The sample mean of a list of values is
\begin{equation}\nonumber
    \overline{x} = \frac{1}{T} \sum_{t=1}^T{\mathbf{v}[t]}.
\end{equation}
The sample standard deviation can be found with the formula
\begin{equation}\nonumber
    s = \sqrt{\frac{1}{T-1} \sum_{t=1}^T{\left(\mathbf{v}[t] - \overline{x} \right)^2}}
\end{equation}
(It should be noted that for applications to whole ECGs, the sample standard
deviation and population standard deviation are very similar, as $T$ is often
$>100.000$). Finally, the normalized time series values can be obtained by
computing
\begin{equation}\nonumber
    \mathbf{v}[t] = \frac{\mathbf{v}[t] - \overline{x}}{s}, \qquad \forall t \in 
    \{1,\ldots,T\}.
\end{equation}
The resulting time series will have the same shape as the raw time series, but
it will have no unit and be normalized.

\TODO{insert figure here}

\paragraph{Dimension reduction with PAA.}\label{08:paa-proc}

The dimension reduction of the SAX representation is due to the use of PAA. The
PAA method takes a univariate time series $\mathbf{v}[t]$ of length $T$ and an
integer $w$ and segments $\mathbf{v}[t]$ into $w$ segments, taking the average
of each. Following \mycite{lin2003}, the resulting representation is denoted as
$\overline{\mathbf{v}}[t]$ and now has length $w$. The PAA representation of
$\mathbf{v}[t]$ can be calculated by using the following formula 
\mycite{lin2003}
\begin{equation}\label{08:eq:paa}
    \overline{\mathbf{v}}[t] = \frac{w}{T} \sum_{j
    = \frac{n}{w}(t-1)+1}^{\frac{n}{w}t}{\mathbf{v}[t]},\qquad \forall t \in
    \{1,\ldots,w\}.
\end{equation}
Now $\mathbf{v}[t]$ has been converted to the PAA representation
$\overline{\mathbf{v}}[t]$.
This process reduces the length of the time series from $T$ to $w$, with the
dimension reduction ratio depending on the choice of $w$.
\TODO{insert figure here}

\paragraph{Discretization of PAA representation.}

This last step in the SAX representation process involves transforming the PAA
representation $\overline{\mathbf{v}}[t]$ into a sequence of equiprobable symbols. Here it is assumed that
a normalized time series has a Gaussian normal distribution ($\mathcal{N}(0,
1)$). The number symbols used is denoted by $a$--the alphabet size. To create 
the equiprobable 
symbols, \textcite{lin2003} use so-called ``breakpoints". These breakpoints are 
a sorted list of numbers $B = \beta_1,\ldots,\beta_{a-1}$. The area under the
normal curve $\mathcal{N}(0,1)$ (i.e. the probability) between two consecutive
segments $\beta_i$ and $\beta_{i+1}=1/a$. This creates $a$ segments ($a-1$
breakpoints) of $\mathcal{N}(0,1)$ that have the same area, i.e. the same 
probability. The values of the breakpoints in $B$ can be found in a Z-table. 
For illustration, \tabref{08:tab:B} shows the breakpoint values for $a=3$ to 
$a=6$.
\begin{table}[t]
    \centering
    \caption{Breakpoint values for numbers of breakpoints $a$ from 3 to 6. The
    parameter $a$ determines into how many equally-sized areas the normal curve 
    $\mathcal{N}(0,1)$ is split. The breakpoints $\beta_i$ delimit the areas.
    Table contents are quoted from \mycite{lin2003}.}
    \label{08:tab:B}
    \vspace{1em}
    \begin{tabular}{|>{\columncolor{tablegray}}c | r | r | r | r |}\hline
        \rowcolor{tablegray}
        {\centering\diagbox{$\beta_i$}{$a$}}& \multicolumn{1}{c|}{3} & 
        \multicolumn{1}{c|}{4} & \multicolumn{1}{c|}{5}  & 
        \multicolumn{1}{c|}{6}   \\\hline
        $\beta_1$   & -0.43 & -0.67  & -0.84 & -0.97    \\\hline
        $\beta_2$   &  0.43 &   0  & -0.25 & -0.43    \\\hline
        $\beta_3$   & \multicolumn{1}{c|}{---} &  0.67  &  0.25 &   0    \\\hline
        $\beta_4$   & \multicolumn{1}{c|}{---} &\multicolumn{1}{c|}{---}&  
        0.84 &  0.43    \\\hline
        $\beta_5$   & \multicolumn{1}{c|}{---} &\multicolumn{1}{c|}{---}&
        \multicolumn{1}{c|}{---}&  0.97    \\\hline
    \end{tabular}
\end{table}

Once the breakpoint values have been determined, the discretization process
begins. The process assigns all PAA segments whose value is below $\beta_1$
the symbol ``a". The PAA segments falling in the area $\beta_1 \le$ and $<
\beta_2$ are assigned ``b". This mapping process is continued, until all PAA
segments are symbolized. Now we have arrived at the SAX representation. The SAX 
representation of $\overline{\mathbf{v}}[t]$ is denoted $\widehat{\mathbf{v}}[t]$ and has 
the same length as $\overline{\mathbf{v}}[t]$ ($w$). Mathematically, the
discretization process is formulated in \mycite{lin2003} as
\begin{equation}\nonumber
    \widehat{ \mathbf{v}}[t] = \text{alpha}_j \quad \text{if  } \beta_{j-1} \le
    \widehat{\mathbf{v}}[t] < \beta_j, \qquad \forall t \in \{1,\ldots,w\}.
\end{equation}
Here $\text{alpha}_j$ is the $j$th letter of the alphabet, i.e.
$\text{alpha}_1=$ ``a", $\text{alpha}_2=$ ``b" \ldots{}
The resulting time series representation has an even more reduced dimension than
PAA because instead of infinitely many possible values for the real-valued PAA 
values, now there are only $a$ different, equiprobable symbols. Thus, the SAX
representation $\widehat{\mathbf{v}}[t]$ has been obtained.
\TODO{insert graph here}

\paragraph{SAX distance measure.}

A distance measure between two SAX representations of the same length is
required to be able to compare them with each other. The SAX distance function
is based on the Euclidean Distance between two time series $\mathbf{v}[t]$ and 
$\mathbf{u}[t]$ is \mycite{lin2003}
\begin{equation}\nonumber
    \text{D}\left(\mathbf{u}[t],\mathbf{v}[t] \right) \equiv \sqrt{\sum_{t=1}^{T}
    {\left(\mathbf{u}[t] - \mathbf{v}[t] \right)^2}}.
\end{equation}
Through the PAA distance as an intermediate step, the authors arrive at
$\text{MINDIST}$ in \eqref{08:eq:sax-mindist}, the SAX distance function that 
returns the minimum distance between the two original time series. It is 
defined as \mycite{lin2003}
\begin{equation}\label{08:eq:sax-mindist}
    \text{MINDIST}\left(\widehat{\mathbf{u}}[t], \widehat{\mathbf{v}}[t] \right) \equiv 
    \sqrt{\frac{T}{w}}
    \sqrt{\sum_{t=1}^{w}{\left(\text{dist}(\widehat{\mathbf{u}}[t], 
    \widehat{\mathbf{v}}[t]) \right)^2}}.
\end{equation}
The function $\text{dist}$ is based on a lookup table that contains the
distances between two symbols. \tabref{08:tab:dist} shows the lookup table for
$a=5$. The values of each table cell are 0 for symbols letters or the absolute
difference of the breakpoints otherwise. The formula
\begin{equation}\label{08:eq:dist-cell}
    \text{cell}_{r,c} = 
    \left\{ \, 
        \begin{aligned}
            0, 
                &\quad \text{if}\,\,\, |r-c| \le 1 \\ 
            \beta_{\text{max}(r,c)-1} - \beta_{\text{min}(r,c)}, 
                &\quad \text{otherwise}
        \end{aligned} 
    \right.
\end{equation}
is used to calculate the values of each cell in \tabref{08:tab:dist} by $r$
(row) and $c$ (column) \mycite{lin2003}.
\begin{table}[t]
    \centering
    \caption{A table for the $\text{dist}$ function for $a=5$. Each cell 
    displays the distance between the symbols denoting its row and column. The
    formula for the cell values is \eqref{08:eq:dist-cell}.}
    \label{08:tab:dist}
    \vspace{1em}
    \begin{tabular}{|>{\columncolor{tablegray}}c | r | r | r | r | r |}\hline
        \rowcolor{tablegray} & \multicolumn{1}{c|}{a} & \multicolumn{1}{c|}{b} 
            & \multicolumn{1}{c|}{c} & \multicolumn{1}{c|}{d} & 
            \multicolumn{1}{c|}{e} \\\hline
        a & -0.43 & -0.67 & -0.84 & -0.97 &  \\\hline
        b &  0.43 &     0 & -0.25 & -0.43 &  \\\hline
        c &     0 &  0.67 &  0.25 &     0 &  \\\hline
        d &     0 &     0 &  0.84 &  0.43 &  \\\hline
        e &     0 &     0 &     0 &  0.97 &  \\\hline
    \end{tabular}
\end{table}
\TODO{inset example of distance between a short segment of SAX stuff, maybe two
ecg segments}

\subsubsection{MSAX}

The Multivariate Symbolic Aggregate Approximation was introduced by
\citeauthor{anacleto2020} in \citeyear{anacleto2020}. It is an extension of SAX
to multivariate time series \mycite{anacleto2020}. It shares the main features
of SAX, but expands them to multivariate time series, such as ECGs--$n$ can be
any integer $\ge1$. A lower 
bound for the MSAX distance function also exists, i.e. distance between two 
MSAX representations is, just as in SAX, guaranteed to be representative of 
the Euclidean Distance between the raw time series. As MSAX builds on the
legacy of SAX and purports to improve upon it, it makes a good research topic.
Further, having only been introduced in \citeyear{anacleto2020}, MSAX is new
and there is still much to be learned about it and its applications. The very
similar performance the authors observed between SAX and MSAX in ECG
applications motivate further research in this area as they note in their
conclusion \mycite{anacleto2020}. Using the MSAX 
representation has the same steps as SAX: normalization, PAA-based dimension
reduction, and discretization. A variation of the $\text{MINDIST}$ function
exists, too.

\paragraph{Normalization.}

The rationale for normalization in the MSAX representation is twofold. Firstly,
the same considerations as for SAX apply with regards to comparing two time
series. Secondly, MSAX utilizes multivariate normalization to take advantage of
the covariance structure of multivariate time series data. To avoid confusion
with the previous section, a multivariate time series shall be denoted as 
$\mathbf{V}[t]$. Multivariate normalization relies on a sample mean vector 
containing the sample mean for each of the time series $\left(V_1[t], \ldots, 
V_n[t] \right)^T$ in $\mathbf{V}[t]$. The sample standard deviation is replaced
by a covariance matrix. The sample mean vector is equivalent to the vector of
expected values $\vec E$, following \mycite{anacleto2020}:
\begin{equation}\nonumber
    E(\mathbf{V}[t]) = \vec E = 
    \begin{bmatrix} 
        \text{mean}(V_1[t]) \\ 
        \vdots \\ 
        \text{mean}(V_n[t]) 
    \end{bmatrix}
    = 
    \begin{bmatrix} 
        \frac{1}{T} \sum_{t=1}^T{V_1[t]} \\
        \vdots \\ 
        \frac{1}{T} \sum_{t=1}^T{V_n[t]} \\
    \end{bmatrix}.
\end{equation}
The covariance matrix, an $n\times n$ matrix, contains the variance of each 
part $\left(V_1[t], \ldots, V_n[t] \right)^T$ of $\mathbf{V}[t]$ on its main 
diagonal, and the covariance
between $i$th and $j$th parts of $\mathbf{V}[t]$ in the $(i,j)$ position. The
general form of a covariance matrix is shown in \eqref{08:eq:cov-mat} below.
The covariance matrix is denoted as $\text{Var}(\mathbf{V}[t])$ or
$\Sigma_{n\times n}$. It is calculated as:
\begin{equation}\label{08:eq:cov-mat}
    \text{Var}(\mathbf{V}[t]) = 
    \Sigma_{n\times n} 
    = 
    \begin{bmatrix}
        \text{cov}(V_1, V_1) & \dots  & \text{cov}(V_1, V_n) \\
        \vdots               & \ddots & \vdots \\
        \text{cov}(V_n, V_1) & \dots  & \text{cov}(V_n, V_n) \\
    \end{bmatrix}.
\end{equation}
The covariance of two time series parts $V_i[t]$ and $V_j[t]$ is defined as the
mean of product of the difference between the values of $V_i[t]$ and its
expected value. The following equation illustrates this process:
\begin{equation}\nonumber
    \text{cov}(V_i[t], V_j[t]) = E\Big( \big[V_i[t] - E(V_i[t])\big] \cdot 
        \big[V_j[t] - E(V_j[t])\big] \Big)
\end{equation}
(Note that $\mathbf{V}[t]$ can be conceptualized as a matrix, with its row
representing the different sub-series and the columns representing specific
values of $t$).
Once $\vec E$ and $\Sigma_{n\times n}$ have been found, the time series 
$\mathbf{V}[t]$ can be normalized by the following formula
\mycite{anacleto2020}:
\begin{equation}\nonumber
    \mathbf{V}[t] = \left(\Sigma_{n\times n}\right)^{-1/2} \left(\mathbf{V}[t]
    - \vec E\right).
\end{equation}
The result will have a mean of zero and uncorrelated variables
\mycite{anacleto2020}.

\TODO{insert figure here, use both leads in both graphs; may turn out a little
large}

\paragraph{Dimension reduction with PAA.}

Dimension reduction using PAA for MSAX is performed in exactly the same way as
for SAX. The procedure outlined in the previous section is applied to 
each of the elements $\left(V_1[t], \ldots, V_n[t] \right)^T$ of the time series
$\mathbf{V}[t]$--equation \eqref{08:eq:paa} is applied to each part. This 
results in
a PAA representation of the original time series $\overline{\mathbf{V}}[t] = \left(
\overline{V_1}[t], \ldots, \overline{V_n}[t]\right)^T$. This process also reduces the length 
of the time series from $T$ to $w$ for each sub-series, with the
dimension reduction ratio depending on the choice of $w$ \mycite{anacleto2020}
\TODO{insert figure here}

\paragraph{Discretization of PAA representation.}

The discretization of the PAA representation for MSAX also works like it does
for SAX. Like in the previous paragraph, the process used in the SAX
representation is applied to each of the sub-time series in $\overline{\mathbf{V}}[t]$ to obtain $\widehat{\mathbf{V}}[t]$. The alphabet size $a$ is the
same for each $V_n[t]$ and the symbols are found in the same way as in the SAX
representation. The breakpoint values are calculated the same and
\tabref{08:tab:B} is as valid for MSAX as it is for SAX. The assigning of
symbols is generally performed in the same way, too. For bivariate time series
($n=2$), $V_1[t]$ could be assigned lowercase symbols (``a", ``b" \ldots{})
while $V_2[t]$ could be assigned uppercase symbols (``A", ``B" \ldots{}). This
has no impact on the method, it is simply a visual aid for the viewer to
distinguish the values. The final MSAX representation $\widehat{\mathbf{V}}[t]$ 
will
consist of one long list of symbols because for each moment $t$ all generated
symbols are combined into a list for this time that represent all sub-time
series at that time.\TODO{add a graph and example here for illustration}

\paragraph{MSAX distance measure.}

The MSAX distance measure expands $\text{MINDIST}$ to multivariate time series.
This is done by adding an additional summarization step to the $\text{MINDIST}$
function. The MSAX distance $\text{MINDIST\_MSAX}$ operates on two MSAX 
representations $\widehat{\mathbf{U}}[t],\widehat{\mathbf{V}}[t]$. Both
representations must have the same length $w$ and same number $n$.
$\text{MINDIST\_MSAX}$ sums the distances between the individual elements
$U_i[t], V_i[t]$ for $i = \overline{1,\ldots,n}$. The following equations
expresses $\text{MINDIST\_MSAX}$ \mycite{anacleto2020}.
\begin{equation}\label{08:eq:msax-mindist}
    \text{MINDIST\_MSAX}
        \left(
            \widehat{\mathbf{U}}[t], \widehat{\mathbf{V}}[t] 
        \right) 
    \equiv 
    \sqrt{\frac{T}{w}}
    \sqrt{
        \sum_{t=1}^{w}{
            \left(
                \sum_{i=1}^{n}{
                    \left(
                        \text{dist}(\widehat{U_i}[t],\widehat{V_i}[t]) 
                    \right)^2
                }
            \right)
        }
    }.
\end{equation}
The function $\text{dist}$ is the same as the SAX function, being based on
equation \eqref{08:eq:dist-cell} and lookup tables like \tabref{08:tab:dist}.
Like $\text{MINDIST}$, $\text{MINDIST\_MSAX}$ also lower bounds the Euclidean
Distance and derives all the same benefits from that.
\TODO{inset example of distance between a short segment of SAX stuff, maybe two
ecg segments}

\subsubsection{HOT SAX}

The Heuristically Ordered Time series using Symbolic Aggregate Approximation is
a discord discovery algorithm introduced by \citeauthor{keogh2005} in 
\citeyear{keogh2005} \mycite{keogh2005}. Discord discovery is the process of
identifying subsections of a time series that are most different to other
segments of the time series, i.e. that have the largest distance to other,
non-intersecting subsegments \mycite{keogh2005}. The standard approach to
discord discovery, comparing all segments to all other segments, is too slow
for application to large datasets as it has a complexity of $O(n^2)$. This
means that for $n$ subsegments, around $n^2$ operations need to be performed.

\begin{itemize}
    \item what is hotsax
    \item its theoretical foundations
    \item advantages, disadvantages
    \item how does it work
\end{itemize}


\TODO{the idea is to use the ECG as it would be recorded or digitized by
anyone, without filtering. see zhang2019 if they did filtering}

\TODO{make a final applications section that explains how HOT SAX will be used
with each of the time series}

\TODO{why can filtering be ignored? put this as further research to investigate
the influence of filtering on this process}

\TODO{give good reasons why I chose the methods and data bases}

\TODO{goals:
\\- reader can assess believability of results
\\- all information necessary to replicate the research
\\- describe all materials, procedure, ect
\\- all the formulae
\\- state all the limitations of the methods and the ones I impose myself
\\- analytical methods and languages
}

\TODO{answer questions:
\\- can someone else accurately replicate the study
\\- can the data be obtained again
\\- are all parts / instruments described with enough accuracy
\\- is the data freely available
\\- can the statistical analysis be repeated
\\- can the algorithms be replicated?
}

\TODO{Sections:
    \\- general overview -> flowchart
    \\- then explain each element of the flowchart one by one
    \\- use formulae etc
    \\- nice amount of tikz graphs
    \\- section on implementation with details and the more important elements
    \\    use another flow chart?
    \\- use graphs to illustrate all important elements
    \\- make a data description section that describes my process of data
    handling; which database
    \\- explain the parameters that the methods have and what they mean
    \\- describe how a got all the data
}

This section explains the methods used in this research. \TODO{create flow
charts for all this shit to make it simpler}. First methods section for the
analytical methods in a mathematical way.


\subsection{Statistical Analysis of Results}

\begin{itemize}
    \item explain true positive, true negative, and so on
    \item explain recall, accuracy, precision, f1
    \item explain why recall was chosen and if that is fair
    \item introduce the correlations that we would expect to find if my
        hypothesis is true and also the ones that would disprove it
    \item which types of correlation, significance testing, and modeling will
        be used and why; what are the justifications
\end{itemize}

\subsection{Implementation}

How I implemented the above stuff. Languages, approaches, hurdles, all the
details needed to reproduce this research. Also mention the simplifications
I chose to make and why: no sliding window, only even divisors, only divisors
within sampling frequency and cutting ECG to even multiple of sampling
frequency.

\subsubsection{ECG acquisition}

flow chart for process

\begin{itemize}
    \item where to download
    \item what exactly are the ECGs
    \item where do they come from
    \item technical parameters of them
    \item the physionet suite
    \item annotations, what they mean, how I can get them, etc
\end{itemize}

\subsubsection{preprocessing}

flow chart for process

\TODO{the codes and constants given for each thing}

\begin{itemize}
    \item how were they preprocessed
    \item physionet suite
    \item my script and what it does and why
    \item problems and limitations of this 
    \item libraries used
\end{itemize}

\subsubsection{SAX}

\TODO{how was the whole data thing handled, how is the data created}

flow chart for process

\begin{itemize}
    \item how was sax implemented
    \item how does HOTSAX work here
    \item libraries used
\end{itemize}

\subsubsection{MSAX}

flow chart for process

\begin{itemize}
    \item how was sax implemented
    \item how does HOTSAX work here
    \item \TODO{point out differences to SAX}
    \item libraries used
\end{itemize}

\subsection{Statistical Evaluation}

\begin{itemize}
    \item reading the data into R
    \item summarizing the data
    \item the summarized data files
    \item libraries used
\end{itemize}

\end{document}
