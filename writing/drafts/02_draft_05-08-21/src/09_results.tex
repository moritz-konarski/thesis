\documentclass[../01_main.tex]{subfiles}

\begin{document}

\section{Results}

\TODO{fix up this section, set proper headers, etc}

\subsection{Implementation}

This subsection is concerned with the implementation of the methods discussed
in the previous section. \TODO{spell out what will be covered in which order}

All code used in the implementation of these methods is available upon request
via email at \begin{verbatim}konarski_m@auca.kg\end{verbatim}.

How I implemented the above stuff. Languages, approaches, hurdles, all the
details needed to reproduce this research. Also mention the simplifications
I chose to make and why: no sliding window, only even divisors, only divisors
within sampling frequency and cutting ECG to even multiple of sampling
frequency.

\subsubsection{The ECG Data}

The ECG data used in this research is the MIT-BIH Arrhythmia Database
\mycite{moody2001,goldberger2000}. This database contains 48 ECG recordings
that are each 30 minutes long. For each of the ECGs, this database contains two
ECG leads and is thus multivariate data. The leads chosen are not the same in
each ECG, they were chosen based on which of the 12 originally recorded ones
best represent the condition of the ECG. A team of cardiologists annotated each
heartbeat in each ECG and determined if it is a normal heartbeat or not. For
example, a beat with the annotation ``N" is a normal beat, while ``A" denotes
an atrial premature beat. The annotations also include non-beat features such
as a change in signal quality, denoted by ``$\sim$", or a rhythm change, which
is denoted as ``+". A full list of annotations and their meaning is available
at \url{https://archive.physionet.org/physiobank/annotations.shtml}. These
annotations make it possible to judge the performance of a discord detection
algorithm, as each detected discord can be checked for correctness using the
provided annotations. While the MIT-BIH database is not the only database that
possesses such annotations, it is one of the most commonly used ones in the
literature (see \mycite{prasad2018,kanani2020,kaur2016,nygaard1998,sivaraks2015,
valupadasu2012,zhang2019}) and it represents a middle ground in a couple
important respects. The databases 48 ECGs are a manageable number, falling in
between the extremes of around 10 and over 100 ECGs. Furthermore, the 30
minutes length represent real-world ECGs better than 10 second excerpts, but
are not as long and analysis-intensive as 24 hour recordings. Lastly, the
MIT-BIH database has a sampling frequency of 360 samples per second, which is
an adequate value \mycite{kligfield2007}. The ECG data in this database is
unfiltered.\par

The ECG data can be downloaded using the PhysioNet website at the url
\url{https://www.physionet.org/content/mitdb/1.0.0/}, or, alternatively, using
the PhysionNet-developed WFDB applications package. This package provides
command line applications to work with PhysionNet data. For each of the
individually numbered ECG records, 4 files exist. The
\begin{verbatim}.hea\end{verbatim} files contain metadata on the ECG record,
including anonymized patient information and the lead names. The
\begin{verbatim}.dat\end{verbatim} files contain the actual ECG recording and
the other two files contain additional information, including the annotations.
Once the ECG recording has been downloaded, the
\begin{verbatim}rdsamp\end{verbatim} command is used to convert the binary ECG
recording files into a more user-friendly comma separated value (CSV) file.
The \begin{verbatim}rdann\end{verbatim} command is then used to create a CSV
file containing the annotations for each of the ECG records. Finally, the ECG
recording data and the annotations can be merged into a single file by using
the time stamps contained in both files. This yields full ECG recordings with
added beat annotations in one file. These files are the basis of all further
methods and analysis performed in this research. The author created a script in
the Julia programming language that performs this process.

\subsubsection{SAX, MSAX, HOT SAX Implementation}

The main program for this research was developed using the Julia programming
language. Julia is a scientific programming language that has similarities to
R, MATLAB, and Python. Julia possesses a rich ecosystem of libraries for
visualization, computation, and data manipulation. For more information, visit
the Julia website at \url{https://julialang.org/}. The following subsection
will detail the steps comprising the discord discovery program.\par

The first step is the selection of the important parameters for the methods.
The user defined parameters are:
\begin{itemize}
    \item the sampling frequency of the ECG data to be analyzed;
    \item the number of PAA segments $w$ used for SAX and MSAX;
    \item the alphabet size $a$ used for SAX and MSAX;
    \item the subsequence length that determines HOT SAX;
    \item the variable $k$ indicating how many discords should be found.
\end{itemize}
These parameters determine all actions the program performs afterward. The
second step is to load a CSV file containing the ECG data and annotations into
the program. Once the ECG file is loaded, it is transformed into a data frame.
A data frame is a type of data structure that can hold heterogeneous data
types, e.g. text and numbers. This step adds important information to the ECG
data. The ECG data frame contains the parameters itemized above to enable
reproduction and analysis of the results, an index range for each PAA segment
so it can be located in the raw ECG, the beat annotations for each PAA segment,
and empty data fields for the results of the analysis with HOT SAX. The next
step is the application of the SAX and MSAX representations. The transformation
of the raw time series data to the symbolic representations is performed in the
same order as discussed earlier in this section, and thanks to the Julia
programming language's ecosystem of libraries, can be easily translated into
code. SAX is applied to each of the ECG leads individually, while MSAX is
applied as designed to both at once. HOT SAX comprises the next step. For MSAX,
the HOT SAX process is performed using the MSAX representation and distance
measure. The method returns a list of distances as well as a list of indices
that indicate which PAA segment has which distance. Depending on the parameter
$k$, only the top $k$ of these discords are returned. These results are then
added to the respective PAA segments in the ECG data frame, adding both the
MSAX distance of the segment as well as a binary indicator of whether or not
the segment was detected as a discord. For SAX the process slightly different.
Because SAX is a univariate representation, it cannot be directly applied to
a bivariate ECG. Thus, SAX is applied to each lead of the ECG separately and
HOT SAX is performed for each representation of each lead. Each set of results
is, like MSAX, a list of indices of PAA segments and a list of their distances.
Each sets of results is also added to the ECG data frame. This time the
detection indicator is quaternary, it represents no detection, detection on the
first lead, detection on the second lead, or detection on both leads. After
both of these processes are completed, the ECG data frame is written to a CSV
file for further analysis. This process can be repeated thousands of times to
create data of different values for the parameters to determine optimal values
and their influence.

\subsubsection{Statistical Analysis of Results}

After completing the computations for different sets of parameters, the results
need to be analyzed. While HOT SAX is not a classifier in the sense of
classifying heartbeats by medical standards, it does classify them into
discords and non-discords. Thus, it is a binary classifier. Binary classifiers
can be evaluated using the well-known True Positive, True Negative, False
Negative, and False Positive values. \tabref{08:tab:bin-class} shows their
relationship.
\begin{table}[t]
    \centering
    \caption{Contingency table showing the relationship between detected
    discords and actual annotated values.}
    \label{08:tab:bin-class}
    \vspace{1em}
    \begin{tabular}{|>{\columncolor{tablegray}}c | c | c |}\hline
        \rowcolor{tablegray}
        {\centering\diagbox{Actual}{Assigned}}& Discord Detected & Non-Discord
        Detected \\\hline
        Is Discord      & True Positive     & False Negative \\\hline
        Is Non-Discord  & False Positive    & True Negative  \\\hline
    \end{tabular}
\end{table}
The values in \tabref{08:tab:bin-class} can be used to calculate many useful
ratios that assist the evaluation of the HOT SAX algorithm. This research uses
the recall value (also known as sensitivity), the accuracy, and the precision.
These ratios are calculated as follows: recall value is defined as
\begin{equation}\nonumber
    \text{Recall} = \frac{\text{True Positive}}{\text{True Positive}
    + \text{False Negative}},
\end{equation}
the accuracy as
\begin{equation}\nonumber
    \text{Accuracy} = \frac{\text{True Positive} + \text{True Negative}}
        {\text{True Positive} + \text{True Negative} + \text{False Positive} +
        \text{False Negative}},
\end{equation}
and the precision as
\begin{equation}\nonumber
    \text{Precision} = \frac{\text{True Positive}}{\text{True Positive}
    + \text{False Positive}}.
\end{equation}
Recall can be understood as a measure of how many of the actual discords were
correctly assigned the label discord. This is the most important measure for the
analysis of HOT SAX applied to ECGs because in a medical scenario, identifying
as many possibly relevant sections of the ECG is more important than being
100\% accurate in their identification. The second most important value is
precision, which can be understood as a measure of how many of the detected
discords are actually discords. While it is more important to identify as many
discords as possible, a 100\% recall rate could be achieved simply by assigning
the label of discord to every element in the time series. Furthermore,
detecting too many non-discords as discords makes it harder to analyze the
actual discords that were highlighted. This of course is not useful, and thus
the precision of HOT SAX needs to be incorporated into the analysis. Lastly,
accuracy is not a very good measure for this particular application, as the
majority of the segments in an ECG are non-discords and HOT SAX only detects
a minority of the segments in an ECG. This leads to a high True Negative rate
and thus a relatively high accuracy, even if HOT SAX did not actually detect
any actual discords. Nonetheless, accuracy is a very common indicator of
classifier performance and will thus be considered.\par

The analysis of the methods was performed using the data whose generation was
discussed above. The analysis was performed using the R programming language.
R is an established statistical and mathematical programming language with
great support for statistical methods and tests. The first step in the analysis
was the processing of the data generated using the Julia program. This
consisted of calculating the True Positive, True Negative, False Negative, and
False Positive values for each parameter combination and each method.
A segment was considered a ``non-discord" if its annotation consisted of an
``N" or nothing ``". The former is obvious; the decision to consider no
annotation (``") a non-discord was made because for certain segments of the
ECGs, no annotations were available. This can happen when the subsequence
length for HOT SAX

All files were analyzed with
respect to the performance of MSAX.


To analyze the performance of HOT SAX with SAX and MSAX as representations,

\begin{itemize}
    \item explain true positive, true negative, and so on
    \item explain recall, accuracy, precision, f1
    \item explain why recall was chosen and if that is fair
    \item introduce the correlations that we would expect to find if my
        hypothesis is true and also the ones that would disprove it
    \item which types of correlation, significance testing, and modeling will
        be used and why; what are the justifications
\end{itemize}


\TODO{the idea is to use the ECG as it would be recorded or digitized by
anyone, without filtering. see zhang2019 if they did filtering}

\TODO{make a final applications section that explains how HOT SAX will be used
with each of the time series}

\TODO{why can filtering be ignored? put this as further research to investigate
the influence of filtering on this process}

\TODO{give good reasons why I chose the methods and data bases}

\TODO{goals:
\\- reader can assess believability of results
\\- all information necessary to replicate the research
\\- describe all materials, procedure, ect
\\- all the formulae
\\- state all the limitations of the methods and the ones I impose myself
\\- analytical methods and languages
}

\TODO{answer questions:
\\- can someone else accurately replicate the study
\\- can the data be obtained again
\\- are all parts / instruments described with enough accuracy
\\- is the data freely available
\\- can the statistical analysis be repeated
\\- can the algorithms be replicated?
}

\TODO{Sections:
    \\- general overview -> flowchart
    \\- then explain each element of the flowchart one by one
    \\- use formulae etc
    \\- nice amount of tikz graphs
    \\- section on implementation with details and the more important elements
    \\    use another flow chart?
    \\- use graphs to illustrate all important elements
    \\- make a data description section that describes my process of data
    handling; which database
    \\- explain the parameters that the methods have and what they mean
    \\- describe how a got all the data
}

This section explains the methods used in this research. \TODO{create flow
charts for all this shit to make it simpler}. First methods section for the
analytical methods in a mathematical way.




\subsection{Statistical Evaluation}

\begin{itemize}
    \item reading the data into R
    \item summarizing the data
    \item the summarized data files
    \item libraries used
\end{itemize}

\TODO{must include:
\\- present the actual results and findings
\\- range of validity
\\- DO NOT INTERPRET
\\- mention positive and negative results
\\- give enough information for others to make their own judgements
\\- use subheadings
\\- key results in clear statements at paragraph beginnings
\\- could be short
\\- 
}

\TODO{Sections
    \\- use confusion matrices for what is is vs what was predicted [p. 44
    anacleto2019]
    \\- compare all the parameters and their influence
    \\- 
}

\subsection{First Run}

\begin{itemize}
    \item parameters for this run
    \item why could I not let it continue
    \item what did this run indicate -> what did I change and modify for the
        next run
    \item keep in mind that the lower recall can be caused by the way I do the
        ECG checking, and that I did not want to assign data to segments that
        did not have it before out of fear that I would invent results.
\end{itemize}

\subsection{Second Run}

\subsection{SAX}

influence and significance of all the major parameters:

\begin{itemize}
    \item k
    \item paa count
    \item subsequence count
    \item alphabet size
    \item which ones seem to be the best
\end{itemize}

\subsection{MSAX}

influence and significance of all the major parameters:

\begin{itemize}
    \item k
    \item paa count
    \item subsequence count
    \item alphabet size
    \item which ones seem to be the best
\end{itemize}

\subsection{MSAX vs SAX}

Comparing SAX to MSAX is done using the recall value defined in
\TODO{reference}. Investigating the correlation between the methods
(represented by a 1 for SAX and a 0 for MSAX), yields the correlation
coefficient of -0.25. This coefficient indicates that for all investigated
parameter combinations, the use of the MSAX method is weakly correlated with an
increase in recall. When a specific set of parameters is selected and the
correlation analysis is repeated, the correlation coeficient is -0.73,
indicating a strong correlation. Here $k=-1$ and paa\_count = 12.

\begin{itemize}
    \item just the results that are gained directly from the data
    \item put results in graphs and tables to make them referencable
\end{itemize}

\end{document}
