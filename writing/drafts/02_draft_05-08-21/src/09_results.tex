\documentclass[../01_main.tex]{subfiles}

\begin{document}

\section{Results and Discussion}
\TODO{restructure this section, put all the data stuff under subsection
results}

\TODO{fix up this section, set proper headers, etc}
\TODO{add in some information on how much data, which parameter combinations,
etc were used}
\TODO{make this free of interpretation}
\TODO{mention ECG filtering}

\subsection{Results}

\subsubsection{Implementation}

This subsection is concerned with the implementation of the methods discussed
in the previous section. First, the ECG data and its preparation will be
discussed, followed by notes on the implementation of the SAX, MSAX, and HOT
SAX methods. Lastly, the process used to analyze the results is discussed.
All code used in the implementation of these methods is available upon request
via email at \Verb+konarski_m@auca.kg+.

\paragraph{The ECG Data}

The ECG data used in this research is the MIT-BIH Arrhythmia Database
\mycite{moody2001,goldberger2000}. This database contains 48 ECG recordings
that are each 30 minutes long. For each of the ECGs, this database contains two
ECG leads and is thus multivariate data. The leads chosen are not the same in
each ECG, they were chosen based on which of the 12 originally recorded ones
best represent the condition of the ECG. A team of cardiologists annotated each
heartbeat in each ECG and determined if it is a normal heartbeat or not. For
example, a beat with the annotation ``N" is a normal beat, while ``A" denotes
an atrial premature beat. The annotations also include non-beat features such
as a change in signal quality, denoted by ``$\sim$", or a rhythm change, which
is denoted as ``+". A full list of annotations and their meaning is available
at \url{https://archive.physionet.org/physiobank/annotations.shtml}. These
annotations make it possible to judge the performance of a discord detection
algorithm, as each detected discord can be checked for correctness using the
provided annotations. While the MIT-BIH database is not the only database that
possesses such annotations, it is one of the most commonly used ones in the
literature (see \mycite{prasad2018,kanani2020,kaur2016,nygaard1998,sivaraks2015,
valupadasu2012,zhang2019}) and it represents a middle ground in a couple
important respects. The databases 48 ECGs are a manageable number, falling in
between the extremes of around 10 and over 100 ECGs. Furthermore, the 30
minutes length represent real-world ECGs better than 10 second excerpts, but
are not as long and analysis-intensive as 24 hour recordings. Lastly, the
MIT-BIH database has a sampling frequency of 360 samples per second, which is
an adequate value \mycite{kligfield2007}. The ECG data in this database is
unfiltered.\par

The ECG data can be downloaded using the PhysioNet website at the url
\url{https://www.physionet.org/content/mitdb/1.0.0/}, or, alternatively, using
the PhysionNet-developed WFDB applications package. This package provides
command line applications to work with PhysionNet data. For each of the
individually numbered ECG records, 4 files exist. The
\Verb_.hea_ files contain metadata on the ECG record,
including anonymized patient information and the lead names. The
\Verb_.dat_ files contain the actual ECG recording and
the other two files contain additional information, including the annotations.
Once the ECG recording has been downloaded, the
\Verb_rdsamp_ command is used to convert the binary ECG
recording files into a more user-friendly comma separated value (CSV) file.
The \Verb_rdann_ command is then used to create a CSV
file containing the annotations for each of the ECG records. Finally, the ECG
recording data and the annotations can be merged into a single file by using
the time stamps contained in both files. This yields full ECG recordings with
added beat annotations in one file. These files are the basis of all further
methods and analysis performed in this research. The author created a script in
the Julia programming language that performs this process. Filtering of the ECG
data is not performed. The rationale behind this is twofold. Firstly, the
combination of PAA and discretization in SAX and MSAX has a smooting effect
that exhibits some of the same properties as filtering. Additionally, filtering
of ECGs adds many more parameters that can be modified to improve the
performance of the methods, which is not desirable for this research as the
methods should depend on the least possible number of parameters for
simplicity. As additional support for this approach, \mycite{zhang2019} can be
considered, which successfully uses SAX in their ECG analysis without
mentioning any filtering performed on the ECG data.

\paragraph{SAX, MSAX, HOT SAX Implementation}

The main program for this research was developed using the Julia programming
language. Julia is a scientific programming language that has similarities to
R, MATLAB, and Python. Julia possesses a rich ecosystem of libraries for
visualization, computation, and data manipulation. For more information, visit
the Julia website at \url{https://julialang.org/}. The following subsection
will detail the steps comprising the discord discovery program.\par

The first step is the selection of the important parameters for the methods.
The user defined parameters are:
\begin{itemize}
    \item the sampling frequency of the ECG data to be analyzed;
    \item the number of PAA segments $w$ used for SAX and MSAX;
    \item the alphabet size $a$ used for SAX and MSAX;
    \item the subsequence length that determines HOT SAX;
    \item the variable $k$ indicating how many discords should be found.
\end{itemize}
These parameters determine all actions the program performs afterward. The
second step is to load a CSV file containing the ECG data and annotations into
the program. Once the ECG file is loaded, it is transformed into a data frame.
A data frame is a type of data structure that can hold heterogeneous data
types, e.g. text and numbers. This step adds important information to the ECG
data. The ECG data frame contains the parameters itemized above to enable
reproduction and analysis of the results, an index range for each PAA segment
so it can be located in the raw ECG, the beat annotations for each PAA segment,
and empty data fields for the results of the analysis with HOT SAX. The next
step is the application of the SAX and MSAX representations. The transformation
of the raw time series data to the symbolic representations is performed in the
same order as discussed earlier in this section, and thanks to the Julia
programming language's ecosystem of libraries, can be easily translated into
code. SAX is applied to each of the ECG leads individually, while MSAX is
applied as designed to both at once. HOT SAX comprises the next step. For MSAX,
the HOT SAX process is performed using the MSAX representation and distance
measure. The method returns a list of distances as well as a list of indices
that indicate which PAA segment has which distance. Depending on the parameter
$k$, only the top $k$ of these discords are returned. These results are then
added to the respective PAA segments in the ECG data frame, adding both the
MSAX distance of the segment as well as a binary indicator of whether or not
the segment was detected as a discord. For SAX the process slightly different.
Because SAX is a univariate representation, it cannot be directly applied to
a bivariate ECG. Thus, SAX is applied to each lead of the ECG separately and
HOT SAX is performed for each representation of each lead. Each set of results
is, like MSAX, a list of indices of PAA segments and a list of their distances.
Each sets of results is also added to the ECG data frame. This time the
detection indicator is quaternary, it represents no detection, detection on the
first lead, detection on the second lead, or detection on both leads. After
both of these processes are completed, the ECG data frame is written to a CSV
file for further analysis. This process can be repeated thousands of times to
create data of different values for the parameters to determine optimal values
and their influence.

\paragraph{Statistical Analysis of Results}

After completing the computations for different sets of parameters, the results
need to be analyzed. While HOT SAX is not a classifier in the sense of
classifying heartbeats by medical standards, it does classify them into
discords and non-discords. Thus, it is a binary classifier. Binary classifiers
can be evaluated using the well-known True Positive, True Negative, False
Negative, and False Positive values. \tabref{08:tab:bin-class} shows their
relationship.
\begin{table}[t]
    \centering
    \caption{Contingency table showing the relationship between detected
    discords and actual annotated values.}
    \label{08:tab:bin-class}
    \vspace{1em}
    \begin{tabular}{|>{\columncolor{tablegray}}c | c | c |}\hline
        \rowcolor{tablegray}
        {\centering\diagbox{Actual}{Assigned}}& Discord Detected & Non-Discord
        Detected \\\hline
        Is Discord      & True Positive     & False Negative \\\hline
        Is Non-Discord  & False Positive    & True Negative  \\\hline
    \end{tabular}
\end{table}
The values in \tabref{08:tab:bin-class} can be used to calculate many useful
ratios that assist the evaluation of the HOT SAX algorithm. This research uses
the recall value (also known as sensitivity), the accuracy, and the precision.
These ratios are calculated as follows: recall value is defined as
\begin{equation}\nonumber
    \text{Recall} = \frac{\text{True Positive}}{\text{True Positive}
    + \text{False Negative}},
\end{equation}
the accuracy as
\begin{equation}\nonumber
    \text{Accuracy} = \frac{\text{True Positive} + \text{True Negative}}
        {\text{True Positive} + \text{True Negative} + \text{False Positive} +
        \text{False Negative}},
\end{equation}
and the precision as
\begin{equation}\nonumber
    \text{Precision} = \frac{\text{True Positive}}{\text{True Positive}
    + \text{False Positive}}.
\end{equation}
Recall can be understood as a measure of how many of the actual discords were
correctly assigned the label discord. This is the most important measure for the
analysis of HOT SAX applied to ECGs because in a medical scenario, identifying
as many possibly relevant sections of the ECG is more important than being
100\% accurate in their identification. The second most important value is
precision, which can be understood as a measure of how many of the detected
discords are actually discords. While it is more important to identify as many
discords as possible, a 100\% recall rate could be achieved simply by assigning
the label of discord to every element in the time series. Furthermore,
detecting too many non-discords as discords makes it harder to analyze the
actual discords that were highlighted. This of course is not useful, and thus
the precision of HOT SAX needs to be incorporated into the analysis. Lastly,
accuracy is not a very good measure for this particular application, as the
majority of the segments in an ECG are non-discords and HOT SAX only detects
a minority of the segments in an ECG. This leads to a high True Negative rate
and thus a relatively high accuracy, even if HOT SAX did not actually detect
any actual discords. Nonetheless, accuracy is a very common indicator of
classifier performance and will thus be considered.\par

The analysis of the methods was performed using the data whose generation was
discussed above. The analysis was performed using the R programming language.
R is an established statistical and mathematical programming language with
great support for statistical methods and tests. The first step in the analysis
was the processing of the data generated using the Julia program. This
consisted of calculating the True Positive, True Negative, False Negative, and
False Positive values for each parameter combination and each method.
A segment was considered a ``non-discord" if its annotation consisted of an
``N" or nothing ``". The former is obvious; the decision to consider no
annotation (``") a non-discord was made because for certain segments of the
ECGs, no annotations were available. This can happen if, for example, the 
subsequence length for HOT SAX is much smaller than one heartbeat. In that
situation, one heartbeat might be represented by 5 or more subsegments. The
heartbeat annotation, given for a specific point in time, will only fall into
one of the 5 segments and can thus only be counted for that one segment. The
same is true for an annotation showing a discord. This method of analysis puts
HOT SAX at a disadvantage because a discord located in one subsegment might
influence its neighboring segments and thus lead to their detection. This
detection might be an actual discord being detected, but counting it as one
would incorrectly inflate the True Positive rate by assuming something about
the data that it itself does not support without some inference. Thus the
decision was made to accept lower True Positive values than may be accurate.
\TODO{explain why? or do so later in limitations section}. Any annotation that
was not empty or ``N" was considered a discord. This includes the medical
annotation for arrhythmia but also the annotations for changes in signal
quality or noise. This is done because HOT SAX is not meant to classify
heartbeats by medical significance, but by how different they are from other
heartbeats. A very noisy normal heartbeat will be detected the same as
a arrhythmic beat. The classification of the detected discords into medically
normal and abnormal heartbeats is left to more sophisticated analysis methods
or human experts. The purpose of the HOT SAX methods is merely to reduce the
number of ECG segments that need to be analyzed by pre-selecting the beats
likely to contain useful information. After calculating the True Positive, 
True Negative, False Negative, and False Positive value for each parameter
combination, they were collected in a data frame also containing information on 
the parameters that lead to them. These data frames are then saved as CSV files
for further analysis. The contingency values were analyzed for the HOT SAX with 
MSAX method, for HOT SAX with individual SAX (only considering a single ECG 
lead), and for a HOT SAX with combined SAX method where the detected discords 
of the individual HOT SAX with SAX computations were combined. The very last
step in the analysis was the calculation of the average recall, precision, and
accuracy across all 48 ECGs for SAX and MSAX. This allows for a simpler
comparison of the results for different parameters and enables pruning of
certain parameter combinations before more sophisticated analysis begins.

\subsection{Limitations}

The limitations of this research are the following: HOT SAX is not a classifier
based on medically relevant information, it classifies discords and not beat
types. This means that its applications to diagnosing heart conditions is
limited. HOT SAX can be used to pre-select specific ECG segments to look at and
analyze because they exhibit discords, but it cannot, by itself, perform any
type of diagnosis. The previous paragraph explains why empty annotations have
to be counted as normal beats and while the author believes that this is 
necessary, it does negatively influence the results. The implementation of the
representation only allows the use of PAA segment numbers that evenly divide
the sampling frequency of the ECG database. This was done so that the whole
ECG, being an even multiple of the sampling frequency itself, can be evenly
divided into PAA segments. This decision prohibits certain numbers of PAA
segments as there may be numbers that do not evenly divide into the sampling
frequency but that do evenly divide the number of raw data points in the ECG.
A further simplification step in the same vein is the restriction of
subsequence values to numbers that evenly divide the number of PAA segments
$w$. This was also done to simplify the process and to guarantee that the whole
ECG would be evenly divisible into subsequences.

\TODO{finish this}\\
\TODO{move this to somewhere else?}
\TODO{mention the 1 second interval connection to the sampling frequency and
why it works}

\subsection{Data Generation 1}

The first round of data generation involved the creation of 126,720 files which
corresponds to 2,640 unique sets of parameters. The following itemization shows
the different parameters that were used in this data generation. Parameter $k$
is number of discords returned by HOT SAX, $w$ the number of PAA segments in
one second of 360 data points; $w$ has to evenly divide 360. Parameter $m$ is
chosen after parameter $w$ and represent the number of PAA segments that are
grouped together to form a HOT SAX subsequence. This parameter must evenly
divide $w$. Lastly, $a$ is the alphabet size which determines the number of
different symbols used in the SAX and MSAX discretization processes.
\begin{itemize}
    \item $k \in \{-1,  25,  50, 100, 150, 200, 300, 500\}$
    \item $w \in \{ 2,  3,  4,  5, 12, 20, 30, 40, 60 \}$
    \item $m \in \{ 2,  3,  4,  5, 12, 20, 30, 40, 60 \}$
    \item $a \in \{ 4,  5,  6,  7,  8,  9, 10, 12, 14, 17, 20\}$
\end{itemize}
Analyzing the 126,720 data files by calculating the mean values of the
statistical measures for each set of unique parameters yields, for HOT SAX with 
MSAX, the recall value was 99.98\% for the parameters 
$k=-1,\,w=60,\,m=60,\,a=20$. Further analysis of rhese results indicate a few 
important things:
\begin{itemize}
    \item the recall value is correlated strongly to $k$, the number of discords
        considered;
    \item the recall value is correlated to the length of the subsequence, the
        longer the subsequence, the higher the recall value is.
\end{itemize}
The second observation is most important. As the addition of different
subsequence length smaller than the 1 second interval does not improve the
recall value, it was decided to generate a second set of data that did not have
subsequence lengths smaller than the interval of 1 second. Instead, a larger
set of the other parameters would be considered. 
\TODO{expand this statement to SAX as well, make subsections}

\subsection{Data Generation 2}

The second round of data generation yielded 238,464 files corresponding to
4,968 unique parameter combinations, each for 48 ECGs. The following list shows
the sets of possible parameters:
\begin{itemize}
    \item $k \in \{-1,  25,  50,  75, 100, 150, 175, 200, 300\}$
    \item $w \in \{  2, 3,   4,   5,   6,   8,   9,  10,  12,  15,  18,  20,  
        24,  30,  36,  40,  45, 60,  72,  90, 120, 180, 360 \}$
    \item $m = w$
    \item $a \in \{ 2,  3,  4 , 5, 6 , 7 , 8 , 9 ,10, 11, 12, 13, 14, 15, 16, 
        17, 18, 19, 20, 21, 22, 23, 24, 25\}$
\end{itemize}
To analyze the performance of MSAX and SAX for this dataset, first the
parameter combinations with an average recall value of 95\% were selected.
For MSAX, this set has 255 values, for single SAX it has 99, and for
combined SAX 192. Then, these values were sorted by their precision. This is 
meant to represent a compromise between recall and precision. 

The top 10 results for each method were chosen for further analysis. For MSAX,
the best precision value is 36.24\% for $k=-1,\,w=m=6,\,a=24$ at a recall value
of 95.37\%. For single SAX, the best precision value is 35.94\% for the
parameters $k=-1,\,w=m=40,\,a=19$ at a recall value of 95.21\%, and for dual
SAX the best precision value is 36.56\% for $k=-1,\,w=m=12,\,a=22$ at a recall
value of 95.28\%. At first glance dual SAX seems to have a slightly higher
precision that MSAX, while MSAX has a slighly higher recall value. Single SAX
performs worse than both other methods. Besides the average value, the 
distribution of recall and precision values are important.
\figref{09:fig:recall} shows a boxplot comparing MSAX and dual SAX with
respect to their recall values. It can be observed that they are virtually
identical, with MSAX having a slightly higher value and being slightly more
tightly grouped than dual SAX. MSAX has more outliers. 
\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{recall}
    \caption{Boxplot comparing the recall value for MSAX and SAX for their
    respective best parameters. A slightly tighter grouping of can be observed
    for the MSAX, while it also has more outliers than the SAX method.}
    \label{09:fig:recall}
\end{figure}
\figref{09:fig:precision} shows a boxplot comparing MSAX and dual SAX with
respect to their precision values. It can be observed that these, too, are 
virtually identical. Dual SAX has a slighly higher precision, while MSAX has
a slighly tighter grouping. There are no outliers.
\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{precision}
    \caption{Boxplot comparing the precision value for MSAX and SAX for their
    respective best parameters. A slightly tighter grouping of can be observed
    for the MSAX, while its average is slighly lower than that of the SAX method.}
    \label{09:fig:precision}
\end{figure}
Performing Pearson correlation analysis with respect to the correlation between
precision and the used method yields a correlation coefficient of 0.006. This
very clearly indicates that there is no significant difference between the two
recall values being influenced by the methods. Performing the same test with
respect to precision yields a correlation coefficient of -0.004, also
indicating no correlation between the methods used analyzed here.

\subsection{Discussion}                                                            
                                                                                
\TODO{expand on this, potentially merge with results section}                   
                                                                                
In the previous section the results of the data analysis were presented. It was 
found that when the optimal parameters for MSAX and dual SAX are used, there is 
no significant difference between the two methods.                              
                                                                                
The most important result is the one mentioned above, as it indicates that, we  
infer, the MSAX representation is equal in performance to the dual SAX method   
when applied to ECGs using HOT SAX. This is congruent with the results of       
\mycite{anacleto2020}, where the authors found only a slight difference between 
the dual SAX method and MSAX. Discussing the actual parameters, MSAX used a PAA 
segment count of 6, representing 60 times reduction in dimension, while dual    
SAX performed best for $w=12$, a 30 times reduction in dimension.    

\end{document}
showed parameter
sets with $k=-1$. The best parameter sets under these conditions had values of
$w$ that were $\le 12$. Only the 7th to 10th best parameter combinations had $w
\ge 12$. \TODO{make a table here}\TODO{do this for single and dual SAX}. As
these values are average values, they have to investigated more closely with
regards to their outliers and interquartile range. The following boxplot can
illustrate the recall and precision rates for these parameter combinations.

\TODO{boxplots of each of the result sections}
\TODO{top 10 table for each of the methods}
\TODO{investigate correlation between sax and msax for those and see if it is
significant}

\end{document}

All files were analyzed with
respect to the performance of MSAX.


To analyze the performance of HOT SAX with SAX and MSAX as representations,

\begin{itemize}
    \item explain true positive, true negative, and so on
    \item explain recall, accuracy, precision, f1
    \item explain why recall was chosen and if that is fair
    \item introduce the correlations that we would expect to find if my
        hypothesis is true and also the ones that would disprove it
    \item which types of correlation, significance testing, and modeling will
        be used and why; what are the justifications
\end{itemize}


\TODO{the idea is to use the ECG as it would be recorded or digitized by
anyone, without filtering. see zhang2019 if they did filtering}

\TODO{make a final applications section that explains how HOT SAX will be used
with each of the time series}

\TODO{why can filtering be ignored? put this as further research to investigate
the influence of filtering on this process}

\TODO{give good reasons why I chose the methods and data bases}

\TODO{goals:
\\- reader can assess believability of results
\\- all information necessary to replicate the research
\\- describe all materials, procedure, ect
\\- all the formulae
\\- state all the limitations of the methods and the ones I impose myself
\\- analytical methods and languages
}

\TODO{answer questions:
\\- can someone else accurately replicate the study
\\- can the data be obtained again
\\- are all parts / instruments described with enough accuracy
\\- is the data freely available
\\- can the statistical analysis be repeated
\\- can the algorithms be replicated?
}

\TODO{Sections:
    \\- general overview -> flowchart
    \\- then explain each element of the flowchart one by one
    \\- use formulae etc
    \\- nice amount of tikz graphs
    \\- section on implementation with details and the more important elements
    \\    use another flow chart?
    \\- use graphs to illustrate all important elements
    \\- make a data description section that describes my process of data
    handling; which database
    \\- explain the parameters that the methods have and what they mean
    \\- describe how a got all the data
}

This section explains the methods used in this research. \TODO{create flow
charts for all this shit to make it simpler}. First methods section for the
analytical methods in a mathematical way.




\subsection{Statistical Evaluation}

\begin{itemize}
    \item reading the data into R
    \item summarizing the data
    \item the summarized data files
    \item libraries used
\end{itemize}

\TODO{must include:
\\- present the actual results and findings
\\- range of validity
\\- DO NOT INTERPRET
\\- mention positive and negative results
\\- give enough information for others to make their own judgements
\\- use subheadings
\\- key results in clear statements at paragraph beginnings
\\- could be short
\\- 
}

\TODO{Sections
    \\- use confusion matrices for what is is vs what was predicted [p. 44
    anacleto2019]
    \\- compare all the parameters and their influence
    \\- 
}

\subsection{First Run}

\begin{itemize}
    \item parameters for this run
    \item why could I not let it continue
    \item what did this run indicate -> what did I change and modify for the
        next run
    \item keep in mind that the lower recall can be caused by the way I do the
        ECG checking, and that I did not want to assign data to segments that
        did not have it before out of fear that I would invent results.
\end{itemize}

\subsection{Second Run}

\subsection{SAX}

influence and significance of all the major parameters:

\begin{itemize}
    \item k
    \item paa count
    \item subsequence count
    \item alphabet size
    \item which ones seem to be the best
\end{itemize}

\subsection{MSAX}

influence and significance of all the major parameters:

\begin{itemize}
    \item k
    \item paa count
    \item subsequence count
    \item alphabet size
    \item which ones seem to be the best
\end{itemize}

\subsection{MSAX vs SAX}

Comparing SAX to MSAX is done using the recall value defined in
\TODO{reference}. Investigating the correlation between the methods
(represented by a 1 for SAX and a 0 for MSAX), yields the correlation
coefficient of -0.25. This coefficient indicates that for all investigated
parameter combinations, the use of the MSAX method is weakly correlated with an
increase in recall. When a specific set of parameters is selected and the
correlation analysis is repeated, the correlation coeficient is -0.73,
indicating a strong correlation. Here $k=-1$ and paa\_count = 12.

\begin{itemize}
    \item just the results that are gained directly from the data
    \item put results in graphs and tables to make them referencable
\end{itemize}

\end{document}
